\documentclass[parskip=full]{scrartcl}

\pdfoutput=1

\title{Finance Reinforcement Learning \\ \LARGE{Trading Agents}}

\author{
	Fernando Bacao\(^{1*}\), Georgios Douzas\(^{1}\), Jorge Antunes\(^{1}\)
	\\
	\small{\(^{1}\)NOVA Information Management School, Universidade Nova de Lisboa}
	\\
	\small{*Corresponding Author: bacao@novaims.unl.pt}
	\\
	\\
	\small{Postal Address: NOVA Information Management School, Campus de Campolide, 1070-312 Lisboa, Portugal}
	\\
	\small{Telephone: +351 21 382 8610}
}

\usepackage{breakcites}
\usepackage{float}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{
	a4paper,
	total={170mm,257mm},
	left=18mm,
	right=18mm,
	top=8mm,
}
\usepackage{amsmath}
\newcommand{\inlineeqnum}{\refstepcounter{equation}~~\mbox{(\theequation)}}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{pgfplotstable}
\usepackage{longtable}
\usepackage{tabu}
\usepackage{hyperref}
\date{}



\begin{document}

\maketitle

\begin{abstract}
Abstract.
\end{abstract}

\section{Introduction}

\section{Related Work}

\section{Motivation}

\section{The Proposed Method}

\section{Research Methodology}

\section{Results and discussion}

\section{Conclusions}

\section{Appendix}

Here we present details of mathematical derivations of the results presented in the
text. The broad ideas and intuitions are discussed there; therefore here we focus on the technical aspects\par

%Here we will find a group of decisions that will help us to define the problem and the \textit{reproducibility} of the article
%\textbf{An adaptive portfolio trading system: A risk-return portfolio optimization using recurrent reinforcement learning with expected maximum drawdown. Almahdi, Yang 2017 }

\subsection{Risk Measures}

As explained in the text, our goal is to measure both profitability and risk exposure. The metrics applied were the Sharpe Ratio and the Calmar Ratio.

The Sharpe Ratio uses the standard deviation of returns as the risk measure.
It is defined as $ Shrp = \frac{\gamma}{\sigma}$

Where $\gamma$ is the mean of the returns and $\sigma$ is the standard deviation of the returns.
% How many n returns? a decision to be made

The Calmar Ratio, like the Sharpe Ratio, is a risk-adjusted measure of performance. Here we derive reward based on this expected maximum drawdown $E(MDD)$ risk-based measure.
Our objective function is deferential of the Calmar Ratio

$$ C_T = \frac{\gamma T}{E(MDD)} $$

where,

\begin{equation*}
E(MDD) = 
	\begin{cases}
		\frac{2\sigma^2}{\gamma}Q_p(\frac{\gamma^2T}{2\sigma^2}) \xrightarrow{T \to \infty}  \frac{\sigma^2}{\gamma}(0.63519 + 0.5\log T + \log\frac{\gamma}{\sigma}) & \text{if}\ \gamma > 0\\ 
		     
		1.2533\sigma \sqrt{T} & \text{if}\ \gamma = 0\\
		
		\frac{-2\sigma^2}{\gamma}Q_n(\frac{\gamma^2T}{2\sigma^2}) \xrightarrow{T \to \infty} -\gamma T - \frac{\sigma^2}{\gamma} & \text{if}\ \gamma < 0
	\end{cases}
\end{equation*}

The functions $Q_n(x)$ and $Q_p(x)$ do not have a convenient analytical form and can be found in \cite{pratap2004maximum} and \cite{magdon2004maximum}.

\subsubsection{Long and Short Positions}

For each algorithmic iteration $ F_t$ gives a negative and positive result which is transformed in a trading signal $F_t \in \{-1,1\}$, short and long positions respectively.

The relevance of the trading signal is the discount of the transactions.

\subsubsection{Return}

Return is obtained (at time $ t $)
$ R_t = \mu \times [F_{t-1}.r_t - \delta|F_t-F_{t-1}|]$

where

\qquad $\mu$ is the number of shares

\qquad$\delta$ is the transaction cost
		
\qquad$r_t$ (log return) is defined as $ r_t = log(price_t) - log(price_{t-1})$

\subsubsection{Objective Function}

As referred in the text, our goal is to maximize the Calmar Ratio. In its most simple form is
$$ C_T = \frac{\frac{T}{2}Shrp^2}{Q_p(\frac{T}{2})} \xrightarrow{T \to \infty}  \frac{T Shrp^2}{0.63519 + 0.5\log T + \log Shrp}$$

Knowing that  $Shrp \equiv S_T = \dfrac{E[R_T]}{\sigma}$

The deferential of the Calmar Ratio is the objective function. through the application of the Chain Rule we get \cite{moody1998performance,almahdi2017adaptive}

$ A = \frac{1}{T}\sum_{t=1}^{T}R_t $

$ B = \frac{1}{T}\sum_{t=1}^{T}R_{t}^{2} $

%As eq. 5 in Almahdi and Yang 2017

$$ \frac{dC_t}{d\theta} = \sum_{t=1}^{T}  \{\frac{dC_T}{dA}\frac{dA}{dR_t}+\frac{dC_T}{dB}\frac{dB}{dR_t}\}.\{\frac{dR_t}{dF_t}\frac{dF_t}{d\theta}+\frac{dR_t}{dF_{t-1}}\frac{dF_{t-1}}{d\theta}\} $$

The resulting partial derivatives are the following:

$ \dfrac{dR_t}{dF_t} = -\mu\delta.sgn|F_t - F_{t-1}| $

$ \dfrac{dR_t}{dF_{t-1}} = -\mu.r_t+\mu\delta.sgn|F_t - F_{t-1}| $

$ \dfrac{dF_t}{d\theta} = (1-tanh(x_{t}^{'}\theta)^2).(x_t + \theta_{M+2}\frac{dF_{t-1}}{d\theta}) $

\qquad equivalent to

$$ \dfrac{dF_t}{d\theta} = (1-\bigg(\dfrac{\mathrm{e}^{2x_{t}^{'}\theta}-1}{\mathrm{e}^{2x_{t}^{'}\theta}+1}\bigg)^2).(x_t + \theta_{M+2}\frac{dF_{t-1}}{d\theta}) $$

$x_t = [1;r_t \dots t_{t-M};F_{t-1}]$ and $\theta \in \Re^{M+2}$


Knowing that (if $ \gamma > 0 $) $$ C_T  = \dfrac{TShrp^2}{0.63519 + 0.5\log T + \log Shrp}$$

and Sharpe Ratio

$$ S_T = \dfrac{\text{Average}(R_t)}{\text{Standard deviation}(R_t)} $$

Our Sharpe Ratio for $ n $ returns is 

$ S_n = \dfrac{A_n}{K_n\sqrt{B_n - A_{n}^{2}}} $ \cite{moody1998performance} with

\qquad$ A_n = \dfrac{1}{n}\sum_{i=1}^{n}R_i $	\qquad$ B_n = \dfrac{1}{n}\sum_{i=1}^{n}R_{i}^{2} $ \qquad	$ K_n = \bigg(\dfrac{n}{n-1}\bigg)^{\frac{1}{2}} $

Since $K_n$ is not relevant analytically we can drop it for sake of simplicity and rewrite the Calmar Ratio (if $ \gamma > 0 $)

$$ C_T  = \dfrac{T\bigg(\dfrac{A_n}{\sqrt{B_n - A_{n}^{2}}}\bigg)^2}{0.63519 + 0.5\log T + \log \bigg(\dfrac{A_n}{\sqrt{B_n - A_{n}^{2}}}\bigg)}$$\\

$$\dfrac{dC_T}{dA_n} = \dfrac{d\Bigg(\dfrac{T\bigg(\dfrac{A_n}{\sqrt{B_n - A_{n}^{2}}}\bigg)^2}{0.63519 + 0.5\log T + \log \bigg(\dfrac{A_n}{\sqrt{B_n - A_{n}^{2}}}\bigg)}\Bigg)}{dA_n}$$

Simplifying the Calmar Ratio $C_T$\\

$\dfrac{T\bigg(\dfrac{A_n}{\sqrt{B_n - A_{n}^{2}}}\bigg)^2}{0.63519 + 0.5\log T + \log \bigg(\dfrac{A_n}{\sqrt{B_n - A_{n}^{2}}}\bigg)}$
$ \equiv$
$\dfrac{TA_{n}^{2}}{(B_n - A_{n}^{2})\bigg(\log \bigg(\dfrac{A_n}{\sqrt{B_n - A_{n}^{2}}}\bigg)+ 0.5\log T + 0.63519\bigg)}$\\

Through the chain rule application, we obtain:\\

$$ \dfrac{dC_T}{dA_n} = \dfrac{200000TB_nA_n\left(100000\ln\left(\frac{A_n}{\sqrt{B_n-A_{n}^{2}}}\right)+50000\ln\left(T\right)+13519\right)}{\left(A_{n}^{2}-B_n\right)^2\left(100000\ln\left(\frac{A_n}{\sqrt{B_n-A_{n}^{2}}}\right)+50000\ln\left(T\right)+63519\right)^2} $$

$$ \dfrac{dC_T}{dB_n} = -\dfrac{100000TA_{n}^{2}\left(100000\ln\left(\frac{A_n}{\sqrt{B_n-A_{n}^{2}}}\right)+50000\ln\left(T\right)+13519\right)}{\left(B_n-A_{n}^{2}\right)^2\left(100000\ln\left(\frac{A_n}{\sqrt{B_n-A_{n}^{2}}}\right)+50000\ln\left(T\right)+63519\right)^2}$$


$$ \dfrac{dA_n}{dR_t} = \dfrac{1}{n}$$

$$ \dfrac{dB_n}{dR_t} = \dfrac{2R_t}{n}$$

when  $ \gamma = 0 $

$$C_T  = \dfrac{\gamma T}{1.2533\sigma\sqrt{T}} \equiv \dfrac{Shrp\sqrt{T}}{1.2533} \equiv   \dfrac{A_n\sqrt{T}}{1.2533\sqrt{B_n - A_{n}^{2}}} $$

when  $ \gamma < 0$

$$C_T  = \dfrac{\gamma T}{-\gamma T - \sigma^2/\gamma} \equiv \dfrac{\gamma^2T}{-\gamma^2T - \sigma^2} \equiv   \dfrac{A_n^2T}{-A_n^2T-\sqrt{B_n - A_{n}^{2}}^2} \equiv \dfrac{A_n^2T}{A_n^2(1-T)-B_n}$$

$\dfrac{dC_T}{dA_n} =-\dfrac{2TB_nA_N}{\left(\left(T-1\right)A_{n}^{2}+B_n\right)^2}
$

$\dfrac{dC_T}{dB_n} =\dfrac{TA_{n}^{2}}{\left(B_n+\left(T-1\right)A_{n}^{2}\right)^2}
$

\subsubsection{Parameters}

In order to update the parameters $\theta$, the following gradient ascent is applied:


$ \theta_{i+1} = \theta_{i} + \rho.\frac{dC_t}{d\theta}$ the $\rho $ is the learning rate


\subsubsection{Weights}

The weights are obtained after the \textit{sofmax} function update

$F_{it} = softmax(f_{it}) = \dfrac{e^{f_{it}}}{\sum_{i=1}^{n}e^{f_{it}}} = \dfrac{e^{logsig(x_{it}^{'}\theta_{i})}}{\sum_{i=1}^{n}e^{logsig(x_{it}^{'}\theta_{i})}}$

\bibliography{references}
\bibliographystyle{apalike}

\end{document}

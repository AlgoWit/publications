\documentclass[parskip=full]{scrartcl}

\pdfoutput=1

\title{Imbalanced Learning in Land Cover Classification  \\ \LARGE{Improving minority class' prediction accuracy using the Geometric SMOTE algorithm}}

\author{
	Georgios Douzas\(^{1}\), Fernando Bacao\(^{1*}\), Joao Fonseca\(^{1}\), Manvel Khudinyan\(^{1}\)
	\\
	\small{\(^{1}\)NOVA Information Management School, Universidade Nova de Lisboa}
	\\
	\small{*Corresponding Author}
	\\
	\\
	\small{Postal Address: NOVA Information Management School, Campus de Campolide, 1070-312 Lisboa, Portugal}
	\\
	\small{Telephone: +351 21 382 8610}
}

\usepackage{breakcites}
\usepackage{float}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{geometry}
\geometry{
	a4paper,
	total={170mm,257mm},
	left=18mm,
	right=18mm,
	top=8mm,
}
\usepackage{amsmath}
\newcommand{\inlineeqnum}{\refstepcounter{equation}~~\mbox{(\theequation)}}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.14}
\usepackage{longtable}
\usepackage{tabu}
\usepackage{hyperref}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Abstract goes here
\end{abstract}

\section{Introduction}

The production of accurate Land Use/Land Cover maps offer unique monitoring
capabilities within the remote-sensing domain \cite{Mellor2015}. LULC maps are
being used for a variety of applications, ranging from environmental
monitoring, land change detection, natural hazard assessment up to agriculture
and water/wetland monitoring \cite{Khatami2016}.

Multispectral images are an important resource to build LULC maps, allowing for
the use of classification algorithms, both supervised and unsupervised, to
automate the production of these maps. Although significant progress has been
made in the use of supervised learning techniques for automatic image
classification \cite{Tewkesbury2015}, the acquisition of labeled training sets
continues to be a bottleneck \cite{Rajan2008}. To build  good and robust
supervised classifiers it is crucial to have a large enough training dataset.
Often, the problem is that different land use classes have very different
levels of coverage, while some are frequent in the training dataset, other can
be limited \cite{Feng2019}. In remote-sensing classification problems, the
number of instances of some classes is often very small when compared with the
dominant classes, this is especially true in LULC data \cite{Williams2009,
Cenggoro2018}.

This imbalance in the representation of the different classes constitutes a
problem for the development of accurate classifiers. This problem is usually
referred to, in the machine learning community, as the imbalanced learning
problem \cite{Chawla2004}. The imbalanced learning problem is even more
difficult to address in multi-class problems, making it one of the most
important challenges in the machine learning research community
\cite{Garcia2018}. Improvements in the representation of small classes in
training datasets can have a significant impact in improving the accuracy and
robustness of classifiers for the production of LULC maps.

An imbalanced learning problem refers to a skewed distribution of data across
classes in both binary and multi-class classification problems \cite{Abdi2016}.
In this situation, the minority classes contribute less to the minimization of
accuracy, the typical objective function, inducing a bias towards the majority
class during the learning process \cite{Douzas2019}. Consequently, as typical
classifier learning methods are designed to work with reasonably balanced
datasets, finding meaningful boundaries becomes a very difficult task
\cite{Saez2016}.

Possible approaches to deal with class imbalance can be divided into three main
groups \cite{Fernandez2013}. 1) Cost-sensitive solutions: adaptations at
algorithmic and/or data level by applying higher misclassification costs for the
examples of the minority class. 2) Algorithmic level solutions: classification
algorithms are adapted or created to reinforce the learning of the positive
class. 3) Data level solutions: A more general approach where class
distributions are rebalanced through the sampling of the data space, thus
diminishing class imbalance. Because this latter method is more general, it is
used for a wide range of applications, thus making it of particular interest.
Regardless, data level solutions carry the disadvantage of yielding an increased
search space.

There are several data level solutions to deal with the imbalanced learning
problem, which can be divided into three groups. While Undersampling algorithms
reduce the size of the majority classes, oversampling algorithms attempt to
even the distributions by generating artificial data for the minority class
\cite{Mellor2015}. A hybrid approach, on the other hand, uses both oversampling
and undersampling techniques.

% Can't find source for oversampling/undersampling methods frequently used...
% Should we include MS-cSV in our experiment, or in this paragraph at all?
% Should I include the limitations of each oversampler?
Studies suggest that the usage of data level solutions in remote sensing
problems to balance class distribution seem to outperform models
trained by randomly drawn samples \cite{Wang2019, Mellor2015}.  Studies
employing oversampling methods such as the Synthetic Minority Over-sampling
Technique (SMOTE) \cite{Chawla2002} seem to consistently yield better results
\cite{Johnson2013, Geib2015} compared to no oversampling. Other studies employ
active learning methods such as Margin Sample by Closest Support Vector
(MS-cSV), as it benefits from avoiding oversampling in dense regions close to
the margin and samples all the feature space equivalently \cite{Tuia2009}.
Although, studies employing data level solutions within the remote sensing
domain seem sparse.

In this paper, we test the performance of oversampling techniques on an
imbalanced dataset using different types of classifiers. The effectiveness of
the Geometric SMOTE (G-SMOTE) \cite{Douzas2019} is demonstrated using SMOTE
\cite{Chawla2002}, Borderline SMOTE \cite{Han2005}, ADASYN \cite{HaiboHe2008},
Random Oversampling and no oversampling as baseline methods. Preliminary results
show that G-SMOTE outperforms every other oversampling technique, for both
overall prediction power and minority class prediction power.

Oversamplers' performance is evaluated through an experimental analysis using
the publicly available LUCAS dataset \cite{Toth2013}. This dataset contains
multispectral reflectance data from European soil, filtered for a region in
Portugal. The experimental procedure includes a comparison of the different
oversamplers using 4 classifiers and \textbf{(...)} evaluation metrics.

This paper is organized in \textbf{(...)} sections: \textbf{(...)}

\section{State of the Art}

Generally in machine learning \cite{Douzas2019} and, particularly, in
remote sensing \cite{Feng2019}, data modification through resampling methods
are broader and the most used approaches to deal with the imbalanced learning
problem. These approaches balance the data in the preprocessing step, thus
eliminating the need for model modification and allowing the user to apply any
standard algorithm. Additionally, resampling methods can be easily adapted to a
multi-class imbalanced data, which is a relevant one for LULC classification
case \cite{Feng2019}. In this section the most relevant studies with the
implementation of resampling methods for remote sensing imbalanced data
classification are presented. The oversampling methods are pointed out.

Some of the existing studies implements the random undersampling method,
which randomly reduces the number of training samples  belonging to the
majority class. However, this method has the disadvantage of information loss
as it discards samples from the majority class  \cite{Feng2019}.
\cite{Feng2018} caries out competitive analysis of  undersamplers and
oversamplers performance for land cover classification with  Rotation Forest
(RoF) ensemble classifier. For the experiment random  undersampling,
undersampling based RoF and number of oversampling techniques  were deployed.
The experiment dataset was 16 class imbalanced data from  hypercritical
imagery. The result analysis reveals the low prediction power of the
undersampling methods as they have always been the lowest for the prediction of
all 16 classes.

Contrary to random undersampling, the random oversampling method benefits from
avoiding information loss. However, this method simply replicates randomly
selected instances of the minority class, consequently, increasing the risk of
over-fitting. \cite{Maxwell2018} reports that balancing data with random
oversampling affects the classification performance differently for different
supervised models. In their paper land cover classification with highly
imbalanced data is carried out with six different supervised classifiers.
Implementation of random oversampling could slightly improve the performance of
Random Forest (RF) and Support Vector Machine (SVM) classifiers. On the other
hand it reduced the overall classification accuracy for classifiers such as
Decision Tree (DT), Artificial Neural Network (ANN), k-Nearest Neighbors (k-NN)
and Boosted DT. The paper shows, that even if random oversampling can affect
positively on the prediction of minority classes, however it can corrupt the
prediction accuracy of a majority class.

Synthetic minority oversampling technique (SMOTE) appears in the resent studies
to be one of the most applied oversampling method to successfully handle with
class imbalance problem in land cover classification with imbalanced remote
sensing data. The variational semi-supervised learning (VSSL) proposed in
\cite{Cenggoro2018} aims to fix the class imbalance problem in LULC mapping.
VSSL is a semi-supervised learning framework consisting of a deep generative
model which allows the model to learn both form labeled and unlabeled data. For
this analysis they used SMOTE technique to balance the data. The result shows
significant performance gain on the producer's accuracy of minority class
prediction compared to the results without any imbalanced learning.

Ensemble learning method together with oversampling algorithms have been
successfully implemented for remote sensing image classification in the resent
years. \cite{Feng2018}, \cite{Feng2019} in their research apply Rootation
Forest ensemble learning algorithm with SMOTE to handle with class imbalance
problem. These papers offer a novel approach of SMOTE based RoF algorithm
for a hyperspectral image classification. The idea behind is to iteratively
balance the class distribution with SMOTE before creating each rotation
decision tree. The experiments results yield about the improved classification
for the minority classes as well as for the whole dataset after balancing data
by oversampling.

A number of studies report of notable improvement in LULC mapping accuracy
while applying SMOTE oversampling with standard algorithms to handle with data
imbalance problem for multi-source remote sensing data. \cite{Johnson2016}
uses OpenStreetMap crowdsourced data and Landsat time series for LULC
classification. Simple implementation of SMOTE oversampler shows improved
classification results for all the supervised classifiers (naive bayes, DT and
RF). \cite{Bogner2018} use RF classifier to detect rare land use classes on
MODIS time series. They use SMOTE oversampler to decrease the imbalance ratio
in the original dataset. The comparison of different classification scenarios
shows the minority classes to be better classified after generating synthetic
instances on the training dataset using SMOTE oversampling. However, the study
reports that non of the minority classes could record as high classification
accuracy as the ones for majority classes. \cite{Panda2018} carry out a simple
implementation of SMOTE with five different classifiers (Nave Bayes, DT, RF,
k-NN, SVM) to predict a land cover maps with 4, 5 and 6 classes. The
experimental output highlights the dominance of prediction power of balanced
data over the original imbalanced data. All 5 classifiers show improved
classification accuracy after applying SMOTE oversampling.

Even if the resent studies demonstrate the usefulness of oversampling methods
for remote sensing applications, they are still having drawbacks. Random
oversampling often leads to the model over-fitting as it simply replicates the
instances for minority class \cite{Feng2019}. SMOTE method can avoid
over-fitting but has the disadvantage of generating noisy data \cite{He2008}.
In order to mitigate this problem many variations of SMOTE has been developed
with the objective of preventing noisy data generation (Chapter 1).

In this study we demonstrate the performance of recently proposed
Geometric-SMOTE oversampling technique developed by \cite{Douzas2019}. The
method appears to overcome the above mentioned problem and increase the LULC
classification accuracy with LUCAS imbalanced dataset. In order to avoid from
noisy data generation, Geometric-SMOTE algorithm defines safe areas around the
minority instances. Afterwards, it expands the minority class area for more
informative synthetic sample generation. In this work the comparative analysis
of different oversampling methods are demonstrated for classification of highly
imbalanced LUCAS dataset. To achieve the goals KNN, DT and Gradient Boosting
classifier (GBC) supervised classifiers have been deployed.

\section{Methodology}

This section explains the evaluation process of G-SMOTE's performance. A
description of the baseline methods, classification algorithms, evaluation
methods, dataset used and experimental procedure is provided.

The implementation of the experimental procedure was based on the Python
programming language, using the Scikit-Learn \cite{Pedregosa2011}, Imbalanced-
Learn \cite{JMLR:v18:16-365} and Geometric-SMOTE \cite{Douzas2019} libraries.
The experiments reported in this paper as well the analysis of the results are
reproducible using the scripts available at \url{https://github.com/AlgoWit/publications}.

\subsection{Baseline methods}

Four oversampling techniques were used in this experiment as baseline methods
along with Geometric SMOTE. Random oversampling was chosen for its simplicity.
SMOTE was selected for being a classic technique and the most popular
oversampling algorithm \cite{Douzas2019}. ADASYN \todo{missing citation} and
Borderline SMOTE \todo{missing citation} were selected for representing popular
modifications of the original SMOTE technique in the selection phase and data
generation mechanism, respectively. Finally, we use no oversampling as an
additional baseline method.

\subsection{Classification algorithms}

The selection of classification algorithms was done according to 3 criteria:
learning type, training time and popularity within the remote sensing
community. We further divide the relevant algorithms into 4 different learning
types: neighbours-based, rule-based, ensemble and generalised linear methods.

For each combination of oversampler and classifier, a grid search will be run
in order to find the optimal parameter settings. The number of fits for a
single combination of oversampler-classifier (represented as $x_a$ and $y_b$,
respectively) is defined as:
\todo{confirm that formulas are correct}
\[
fits_{x_a,y_b}=\prod\limits_{p=1} \textrm{count}(x_a^p).\prod\limits_{q=1}
\textrm{count}(y_b^q)
\]

Where $\textrm{count}(x_a^p)$ corresponds to the search grid size of
hyperparameter $p$ of an oversampler $x_a$. Finally, the total number of fits
is defined as:

\[
Total\ Fits=\sum\limits_{a=1} \sum\limits_{b=1} fits_{x_a,y_b}
\]

Thus, in order to setup an extensive grid search and run the full experiment
within feasible time, selecting computationally efficient classifiers becomes
vital.

% popularity of algorithms here
\cite{Khatami2016b}


\subsubsection{neighbours-based}
Amongst the neighbours-based learning methods, K-nearest neighbours (KNN)
\todo{missing citation} algorithm was selected.

\subsubsection{rule-based}
The decision tree classifier (DT) \todo{missing citation} was chosen amongst
the rule-based learning methods.

\subsubsection{ensemble}
Out of the ensemble learning methods, the Gradient boosting classifier
\todo{missing citation} and Random Forest classifier was selected.

\subsubsection{generalised linear methods}
Finally, the logistic regression classifier \todo{missing citation} was
selected amongst the generalised linear models.

% explain why these algorithms have been chosen within each family of
% classifiers

\subsection{Evaluation methods}

Amongst the many evaluation metrics existing for classifier's performance
evaluation, there are several indices that are considered to be preliminary and
are widely used for LULC classification accuracy assessment. Those measures are
classification overall accuracy, user's accuracy or precision and
producer's accuracy or recall,  and are calculated using the classification
confusion matrix \cite{Liu2007}. However, the LUCAS dataset presents high
inter-class variability. Hence, the minority classes can be difficult to
account for by the classifiers that are designed to maximise the overall
accuracy, as the majority classes are receiving more weight \cite{Inglada2017}.
On the other hand, \cite{He2008} states that the use of user's and producer's
accuracy for class imbalanced data is not straightforward and producer's
accuracy is not sensitive to the data distribution at all. Hence, class
imbalance specific metrics are needed to better describe classifier's
performance. For this purpose, F-score and G-mean metrics are used to evaluate
and compare the performance of the oversampling methods. Classification Overall
Accuracy is provided to summarise the confusion matrix.


\subsubsection{Overall Accuracy}

Classification Overall Accuracy is the number of correctly classified samples
divided by the sum of all the samples on the confusion matrix.

\subsubsection{F-score}

F-score (or F-measure) is the harmonic mean of user's accuracy (precision) and
producer's accuracy (recall), where user's accuracy is the fraction of correctly
classified instances with regard to all instances classified as this certain
class in the confusion matrix, and producer's accuracy is the fraction of
correctly classified instances with regard to all instances of that reference
class. F-score can be calculated as for the all classification as well as for
each class. The worst result is at the value of 0 and the best at 1
\cite{Inglada2017}.

\[
F{-}score=2\frac{\text{UA} \times \text{PA}}{\text{UA} + \text{PA}}
\]

Where UA and PA are user's and producer's accuracy respectively.

\subsubsection{G-measure}

G-mean measure is the geometric mean between sensitivity and specificity.

\[G{-}mean = \sqrt{Sensitivity \times Specificity}\]

Where sensitivity is the same user's accuracy or recall, while specificity is
the proportion of actual negatives that are correctly identified as such for
each class.

\subsection{Experiment Settings}

\subsection{Data Set}

For this research LUCAS dataset and Landsat 8 time series from 2015 were used
for training and validation of supervised classifiers. 1694 LUCAS points are
used as reference ground-truth data. This dataset is grouped in 8 classes and
represents the land cover types for the study area. The area of study is the
central part of Portugal. From the Table 1 it is obvious that the data is
highly imbalanced with a minority class of 4 instances (wetlands) and majority
class of 761 instances (woodland).

The remote sensing dataset presents 8 images from Landsat 8ETM+ multi-spectral
sensor. The images are Level-1 terrain corrected products (L1TP) and are
acquired in 2015 form February to September, 1 image each month. Only bands 2,
3, 4, 5, 6 and 7 are used from each image. Thus, each reference point from
LUCAS dataset has 48 features (Table 2). Those features have the value of
digital numbers of the images representing LUCAS point location.

\begin{table}[H]
	\centering
	\begin{tabular}{|p{2.2cm}|p{2.3cm}|p{2cm}|p{2cm}|}
		\hline
		\textbf{LUCAS Category} & \textbf{Land cover type} & \textbf{Instances}
		& \textbf{Imbalance Ratio} \\
		\hline
		A & Artificial land& 131 & 5.81 \\
		\hline
		B & Cropland & 270 & 2.81 \\
		\hline
		C & Woodland & 761 & 1.00 \\
		\hline
		D & Shrubland & 296 & 2.61 \\
		\hline
		E & Grassland & 185 & 4.11 \\
		\hline
		F & Bareland & 37 & 20.56 \\
		\hline
		G & Water & 10 & 76.10 \\
		\hline
		H & Wetlands & 4 &  190.25\\
		\hline
		%\textbf{Total} & & \textbf{1694} &  \textbf{190.25} \\
		%\hline
	\end{tabular}
	\caption{\label{tab:dataset_classes}LUCAS nomenclature and sample distribution}
\end{table}

\begin{table}[H]
	\centering
	\begin{tabular}{|p{2.7cm}|p{3cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
		\hline
		\textbf{Dataset name} & \textbf{Features} & \textbf{Instances} &
		\textbf{Minority instances} & \textbf{Majority instances}	&
		\textbf{Imbalance Ratio} \\
		\hline
		LUCAS dataset & 48 & 1694 & 4 & 761 & 190.25 \\
		\hline
	\end{tabular}
	\caption{\label{tab:datasets}Description of the datasets}
\end{table}

\begin{table}[H]
	\centering
	\begin{tabular}{ll}
		\toprule
		\textbf{Dataset name}             &  LUCAS dataset \\
		\textbf{Features}                 &             48 \\
		\textbf{Instances}                &           1694 \\
		\textbf{Minority class instaces}  &              4 \\
		\textbf{Majority class instances} &            761 \\
		\textbf{Imbalance Ratio}         &         190.25 \\
	\bottomrule
	\end{tabular}
	\caption{\label{tab:datasets2}Alternative for Description of the datasets}
\end{table}

\section{Results and Discussion}

\section{Conclusions}



\bibliography{references}
\bibliographystyle{apalike}

\end{document}

\documentclass[parskip=full]{scrartcl}

\pdfoutput=1

\title{Imbalanced Learning in Land Cover Classification  \\ \LARGE{Improving minority class' prediction accuracy using the Geometric SMOTE algorithm}}

\author{
	Georgios Douzas\(^{1}\), Fernando Bacao\(^{1*}\), Joao Fonseca\(^{1}\), Manvel Khudinyan\(^{1}\)
	\\
	\small{\(^{1}\)NOVA Information Management School, Universidade Nova de Lisboa}
	\\
	\small{*Corresponding Author}
	\\
	\\
	\small{Postal Address: NOVA Information Management School, Campus de Campolide, 1070-312 Lisboa, Portugal}
	\\
	\small{Telephone: +351 21 382 8610}
}

\usepackage{breakcites}
\usepackage{float}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{geometry}
\geometry{
	a4paper,
	total={170mm,257mm},
	left=18mm,
	right=18mm,
	top=8mm,
}
\usepackage{amsmath}
\newcommand{\inlineeqnum}{\refstepcounter{equation}~~\mbox{(\theequation)}}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.14}
\usepackage{longtable}
\usepackage{tabu}
\usepackage{hyperref}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Abstract goes here
\end{abstract}

\section{Introduction}

The production of accurate Land Use/Land Cover maps offer unique monitoring
capabilities within the remote-sensing domain \cite{Mellor2015}. LULC maps are
being used for a variety of applications, ranging from environmental
monitoring, land change detection, natural hazard assessment up to agriculture
and water/wetland monitoring \cite{Khatami2016}.

Multispectral images are an important resource to build LULC maps, allowing for
the use of classification algorithms, both supervised and unsupervised, to
automate the production of these maps. Although significant progress has been
made in the use of supervised learning techniques for automatic image
classification \cite{Tewkesbury2015}, the acquisition of labeled training sets
continues to be a bottleneck \cite{Rajan2008}. To build  good and robust
supervised classifiers it is crucial to have a large enough training dataset.
Often, the problem is that different land use classes have very different
levels of coverage, while some are frequent in the training dataset, other can
be limited \cite{Feng2019}. In remote-sensing classification problems, the
number of instances of some classes is often very small when compared with the
dominant classes, this is especially true in LULC data \cite{Williams2009,
Cenggoro2018}.

This imbalance in the representation of the different classes constitutes a
problem for the development of accurate classifiers. This problem is usually
referred to, in the machine learning community, as the imbalanced learning
problem \cite{Chawla2004}. The imbalanced learning problem is even more
difficult to address in multi-class problems, making it one of the most
important challenges in the machine learning research community
\cite{Garcia2018}. Improvements in the representation of small classes in
training datasets can have a significant impact in improving the accuracy and
robustness of classifiers for the production of LULC maps.

An imbalanced learning problem refers to a skewed distribution of data across
classes in both binary and multi-class classification problems \cite{Abdi2016}.
In this situation, the minority classes contribute less to the minimization of
accuracy, the typical objective function, inducing a bias towards the majority
class during the learning process \cite{Douzas2019}. Consequently, as typical
classifier learning methods are designed to work with reasonably balanced
datasets, finding meaningful boundaries becomes a very difficult task
\cite{Saez2016}.

Possible approaches to deal with class imbalance can be divided into three main
groups \cite{Fernandez2013}. 1) Cost-sensitive solutions: adaptations at
algorithmic and/or data level by applying higher misclassification costs for the
examples of the minority class. 2) Algorithmic level solutions: classification
algorithms are adapted or created to reinforce the learning of the positive
class. 3) Data level solutions: A more general approach where class
distributions are rebalanced through the sampling of the data space, thus
diminishing class imbalance. Because this latter method is more general, it is
used for a wide range of applications, thus making it of particular interest.
Regardless, data level solutions carry the disadvantage of yielding an increased
search space.

There are several data level solutions to deal with the imbalanced learning
problem, which can be divided into three groups. While Undersampling algorithms
reduce the size of the majority classes, oversampling algorithms attempt to
even the distributions by generating artificial data for the minority class
\cite{Mellor2015}. A hybrid approach, on the other hand, uses both oversampling
and undersampling techniques.

% Can't find source for oversampling/undersampling methods frequently used...
% Should we include MS-cSV in our experiment, or in this paragraph at all?
% Should I include the limitations of each oversampler?
Studies suggest that the usage of data level solutions in remote sensing
problems to balance class distribution seem to outperform models
trained by randomly drawn samples \cite{Wang2019, Mellor2015}.  Studies
employing oversampling methods such as the Synthetic Minority Over-sampling
Technique (SMOTE) \cite{Chawla2002} seem to consistently yield better results
\cite{Johnson2013, Geib2015} compared to no oversampling. Other studies employ
active learning methods such as Margin Sample by Closest Support Vector
(MS-cSV), as it benefits from avoiding oversampling in dense regions close to
the margin and samples all the feature space equivalently \cite{Tuia2009}.
Although, studies employing data level solutions within the remote sensing
domain seem sparse.

In this paper, we test the performance of oversampling techniques on an
imbalanced dataset using different types of classifiers. The effectiveness of
the Geometric SMOTE (G-SMOTE) \cite{Douzas2019} is demonstrated using SMOTE
\cite{Chawla2002}, Borderline SMOTE \cite{Han2005}, ADASYN \cite{HaiboHe2008},
Random Oversampling and no oversampling as baseline methods. Preliminary results
show that G-SMOTE outperforms every other oversampling technique, for both
overall prediction power and minority class prediction power.

Oversamplers' performance is evaluated through an experimental analysis using
the publicly available LUCAS Soil dataset \cite{Toth2013}. This dataset contains
multispectral reflectance data from European soil, filtered for a region in
Portugal. The experimental procedure includes a comparison of the different
oversamplers using 4 classifiers and \textbf{(...)} evaluation metrics.

This paper is organized in \textbf{(...)} sections: \textbf{(...)}

\section{State of the Art}

Generally in machine learning \cite{Douzas2019} and, particularly, in
remote sensing \cite{Feng2019}, data modification through resampling methods
are broader and the most used approaches to deal with the imbalanced learning
problem. These approaches balance the data in the preprocessing step, thus
eliminating the need for model modification and allowing the user to apply any
standard algorithm. Additionally, resampling methods can be easily adapted to a
multi-class imbalanced data, which is a relevant one for LULC classification
case \cite{Feng2019}. In this section the most relevant studies with the
implementation of resampling methods for remote sensing imbalanced data
classification are presented. The oversampling methods are pointed out.

Some of the existing studies implements the random undersampling method,
which randomly reduces the number of training samples  belonging to the
majority class. However, this method has the disadvantage of information loss
as it discards samples from the majority class  \cite{Feng2019}.
\cite{Feng2018} caries out competitive analysis of  undersamplers and
oversamplers performance for land cover classification with  Rotation Forest
(RoF) ensemble classifier. For the experiment random  undersampling,
undersampling based RoF and number of oversampling techniques  were deployed.
The experiment dataset was 16 class imbalanced data from  hypercritical
imagery. The result analysis reveals the low prediction power of the
undersampling methods as they have always been the lowest for the prediction of
all 16 classes.

Contrary to random undersampling, the random oversampling method benefits from
avoiding information loss. However, this method simply replicates randomly
selected instances of the minority class, consequently, increasing the risk of
over-fitting. \cite{Maxwell2018} reports that balancing data with random
oversampling affects the classification performance differently for different
supervised models. In their paper land cover classification with highly
imbalanced data is carried out with six different supervised classifiers.
Implementation of random oversampling could slightly improve the performance of
Random Forest (RF) and Support Vector Machine (SVM) classifiers. On the other
hand it reduced the overall classification accuracy for classifiers such as
Decision Tree (DT), Artificial Neural Network (ANN), k-Nearest Neighbors (k-NN)
and Boosted DT. The paper shows, that even if random oversampling can affect
positively on the prediction of minority classes, however it can corrupt the
prediction accuracy of a majority class.

Synthetic minority oversampling technique (SMOTE) appears in the resent studies
to be one of the most applied oversampling method to successfully handle with
class imbalance problem in land cover classification with imbalanced remote
sensing data. The variational semi-supervised learning (VSSL) proposed in
\cite{Cenggoro2018} aims to fix the class imbalance problem in LULC mapping.
VSSL is a semi-supervised learning framework consisting of a deep generative
model which allows the model to learn both form labeled and unlabeled data. For
this analysis they used SMOTE technique to balance the data. The result shows
significant performance gain on the producer's accuracy of minority class
prediction compared to the results without any imbalanced learning.

Ensemble learning method together with oversampling algorithms have been
successfully implemented for remote sensing image classification in the resent
years. \cite{Feng2018}, \cite{Feng2019} in their research apply Rootation
Forest ensemble learning algorithm with SMOTE to handle with class imbalance
problem. These papers offer a novel approach of SMOTE based RoF algorithm
for a hyperspectral image classification. The idea behind is to iteratively
balance the class distribution with SMOTE before creating each rotation
decision tree. The experiments results yield about the improved classification
for the minority classes as well as for the whole dataset after balancing data
by oversampling.

A number of studies report of notable improvement in LULC mapping accuracy
while applying SMOTE oversampling with standard algorithms to handle with data
imbalance problem for multi-source remote sensing data. \cite{Johnson2016}
uses OpenStreetMap crowdsourced data and Landsat time series for LULC
classification. Simple implementation of SMOTE oversampler shows improved
classification results for all the supervised classifiers (naive bayes, DT and
RF). \cite{Bogner2018} use RF classifier to detect rare land use classes on
MODIS time series. They use SMOTE oversampler to decrease the imbalance ratio
in the original dataset. The comparison of different classification scenarios
shows the minority classes to be better classified after generating synthetic
instances on the training dataset using SMOTE oversampling. However, the study
reports that non of the minority classes could record as high classification
accuracy as the ones for majority classes. \cite{Panda2018} carry out a simple
implementation of SMOTE with five different classifiers (Nave Bayes, DT, RF,
k-NN, SVM) to predict a land cover maps with 4, 5 and 6 classes. The
experimental output highlights the dominance of prediction power of balanced
data over the original imbalanced data. All 5 classifiers show improved
classification accuracy after applying SMOTE oversampling.

Even if the resent studies demonstrate the usefulness of oversampling methods
for remote sensing applications, they are still having drawbacks. Random
oversampling often leads to the model over-fitting as it simply replicates the
instances for minority class \cite{Feng2019}. SMOTE method can avoid
over-fitting but has the disadvantage of generating noisy data \cite{He2008}.
In order to mitigate this problem many variations of SMOTE has been developed
with the objective of preventing noisy data generation (Chapter 1).

In this study we demonstrate the performance of recently proposed
Geometric-SMOTE oversampling technique developed by \cite{Douzas2019}. The
method appears to overcome the above mentioned problem and increase the LULC
classification accuracy with LUCAS imbalanced dataset. In order to avoid from
noisy data generation, Geometric-SMOTE algorithm defines safe areas around the
minority instances. Afterwards, it expands the minority class area for more
informative synthetic sample generation. In this work the comparative analysis
of different oversampling methods are demonstrated for classification of highly
imbalanced LUCAS dataset. To achieve the goals KNN, DT and Gradient Boosting
classifier (GBC) supervised classifiers have been deployed.

\section{Methodology}

In this section, we describe the evaluation process of G-SMOTE using various
classifiers, metrics and baseline methods. A description of the baseline
methods, classification algorithms, evaluation methods, dataset used and
experimental procedure is provided.

The implementation of the experimental procedure was based on the Python
programming language, using the Scikit-Learn \cite{Pedregosa2011}, Imbalanced-
Learn \cite{JMLR:v18:16-365} and Geometric-SMOTE \cite{Douzas2019} libraries.
The experiments reported in this paper as well the analysis of the results are
reproducible using the scripts available at \url{https://github.com/AlgoWit/publications}.

\subsection{Baseline methods}

Four oversampling techniques were used in this experiment
as baseline methods along with Geometric SMOTE. Random oversampling was chosen
for its simplicity. SMOTE was selected for being a classic technique and the
most popular oversampling algorithm \cite{Douzas2019}. ADASYN and Borderline
SMOTE were selected for representing popular modifications of the original
SMOTE technique in the selection phase and data generation mechanism,
respectively. Finally, we consider no oversampling as an additional baseline
method.

\subsection{Classification algorithms}

Relevant classification algorithms were selected according to its learning
type. Amongst the neighbours-based learning methods, K-nearest neighbours (KNN)
\todo{missing citation} algorithm was selected. The decision tree classifier
(DT) \todo{missing citation} was chosen amongst the rule-based learning
methods. Out of the ensemble learning methods, the Gradient boosting classifier
\todo{missing citation} was selected. Finally, the logistic regression
classifier \todo{missing citation} was selected amongst the generalised linear
models.

\subsection{Evaluation methods}

Amongst the many evaluation metrics existing for classifier's performance
evaluation, there are several indices that are considered to be preliminary and
fundamental for LULC classification accuracy assessment. Those measures are
classification overall accuracy (OA), user's accuracy or precision (UA) and
producer's accuracy or recall (PA) and are calculated using the classification
confusion matrix \cite{Liu2007}. However, the LUCAS Soil dataset presents high
inter-class variability. Hence, the minority classes can be difficult to
account for by the classifiers that are designed to maximise the overall
accuracy, as the majority classes are receiving more weight \cite{Inglada2017}.
As it is important to consider class imbalance and classification accuracy in
the minority classes, Overall Accuracy together with Kappa statistics and
F-score metrics will be summarized using the information from the confusion
matrix.

% Formulas to be added once the metrics are selected
\textbf{Overall Accuracy}

Classification Overall Accuracy is the proportion of the all samples classified
correctly.

\textbf{Kappa statistics}

Kappa statistics (k) measures the proportion of correctly classified units after
considering the probability of chance agreement and is well correlated to area
under the curve of ROC \cite{Freeman2012}. In other words, Kappa coefficient
indicates the agreement between observed agreement and the random agreement,
that might be expected in case of random classifier.

\textbf{F-score}

F-score (or F-measure) is the harmonic mean of UA (fraction of correctly
classified instances with regard to all instances classified as this certain
class in the confusion matrix) and PA (fraction of correctly classified
instances with regard to all instances of that reference class). F-score can be
calculated as for the all classification as well as for each class. The worst
result is at 0 and the best at the value of 1 \cite{Inglada2017}.

\subsection{Experiment Settings}

\subsection{Data Set}

\section{Results and Discussion}

\section{Conclusions}



\bibliography{references}
\bibliographystyle{apalike}

\end{document}

\documentclass[parskip=full]{scrartcl}

\pdfoutput=1

\title{Imbalanced Learning in Land Cover Classification  \\ \LARGE{Improving minority class' prediction accuracy using the Geometric SMOTE algorithm}}

\author{
	Georgios Douzas\(^{1}\), Fernando Bacao\(^{1*}\), Jo√£o Fonseca\(^{1}\), Manvel Khudinyan\(^{1}\)
	\\
	\small{\(^{1}\)NOVA Information Management School, Universidade Nova de Lisboa}
	\\
	\small{*Corresponding Author}
	\\
	\\
	\small{Postal Address: NOVA Information Management School, Campus de Campolide, 1070-312 Lisboa, Portugal}
	\\
	\small{Telephone: +351 21 382 8610}
}

\usepackage{breakcites}
\usepackage{float}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{
	a4paper,
	total={170mm,257mm},
	left=18mm,
	right=18mm,
	top=8mm,
}
\usepackage{amsmath}
\newcommand{\inlineeqnum}{\refstepcounter{equation}~~\mbox{(\theequation)}}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.14}
\usepackage{longtable}
\usepackage{tabu}
\usepackage{hyperref}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Classification of imbalanced datasets is a challenging task for standard
algorithms. Although many methods exist to address this problem in different
ways, generating artificial data for the minority class is a more general
approach compared to algorithmic modifications. SMOTE algorithm, as well as any
other oversampling method based on the SMOTE mechanism, generates synthetic
samples along line segments that join minority class instances. In this paper we
propose Geometric SMOTE (G-SMOTE) as a enhancement of the SMOTE data generation
mechanism. G-SMOTE generates synthetic samples in a geometric region of the
input space, around each selected minority instance. While in the basic
configuration this region is a hyper-sphere, G-SMOTE allows its deformation to a
hyper-spheroid. The performance of G-SMOTE is compared against SMOTE as well as
baseline methods. We present empirical results that show a significant
improvement in the quality of the generated data when G-SMOTE is used as an
oversampling algorithm. An implementation of G-SMOTE is made available in the
Python programming language.
\end{abstract}

\section{Introduction}

Accurate Land Use/Land Cover (LULC) classification maps play an important role
in the remote-sensing domain \cite{Cenggoro2018} and are a crucial source of
information for natural resource land managers and forest monitoring programs
\cite{Mellor2015}. This data is being used for a variety of applications,
ranging from environmental monitoring, land change detection, natural hazard
assessment up to agriculture and water/wetland monitoring \cite{Khatami2016}.

% Check if this part
While the human eye is based on three color bands (red, green and blue),
spectral imaging divides the vision into many more. The multi/hyper-spectral
context of LULC maps allow the discrimination of pixels among classes with
similar spectral signatures \cite{Marconcini2009}, proving especially useful for
classification tasks. However, datasets that serve this purpose often suffer
from the class imbalanced problem \cite{Feng2019}. In multi-class classification
tasks, this problem is considerably more difficult to address when compared to
binary classification problems, making it one of the most important challenges
in the machine learning research community \cite{Garcia2018}.

In remote-sensing classification problems, the number of targets present is
often very small compared with the remaining classes, especially when using LULC
data \cite{Williams2009, Cenggoro2018}. As an example, in land-mine detection
applications, it is common to have 100 false alarms for each real mine present.

An imbalanced learning problem refers to a skewed distribution of data across
classes in both binary and multi-class classification problems \cite{Abdi2016}.
In this situation, the minority classes contribute less to the minimization of
the objective function, which induces a bias towards the majority class during
the learning process \cite{Douzas2019}. Consequently, as typical classifier
learning methods are designed to work with reasonably balanced datasets, finding
meaningful boundaries becomes a very difficult task \cite{Saez2016}.

% How to justify the usage of the third method? Is it okay this way?
Possible approaches to deal with class imbalance can be divided into three main
groups \cite{Fernandez2013}. 1) Cost-sensitive solutions: adaptations at
algorithmic and/or data level by applying higher misclassification costs for the
examples of the minority class. 2) Algorithmic level solutions: classification
algorithms are adapted or created to reinforce the learning of the positive
class. 3) Data level solutions: A more general approach where class
distributions are rebalanced through the sampling of the data space, thus
diminishing class imbalance. Because this latter method is more general, it is
used for a wide range of applications, thus making it of particular interest.
Regardless, data level solutions carry the disadvantage of yielding an increased
search space. % \cite{Fernandez2013}.

There are several data level solutions to deal with the imbalanced learning
problem, which can be divided into three groups. Undersampling algorithms reduce
the size of the majority classes, whereas oversampling algorithms attempt to
attribute a higher weight to minority classes by generating artificial data
\cite{Mellor2015}. A hybrid approach, on the other hand, uses both oversampling
and undersampling techniques.

% Can't find source for oversampling/undersampling methods frequently used...
% Should we include MS-cSV in our experiment, or in this paragraph at all?
% Should I include the limitations of each oversampler?
Studies suggest that the usage of data level solutions in remote sensing
problems to balance class distribution seem to outperform the ones models
trained by randomly drawn samples \cite{Wang2019, Mellor2015}.  Studies
employing oversampling methods such as the Synthetic Minority Over-sampling
Technique (SMOTE) \cite{Chawla2002} seem to consistently yield better results
\cite{Johnson2013, Geib2015} compared to no oversampling. Other studies employ
active learning methods such as Margin Sample by Closest Support Vector
(MS-cSV), as it benefits from avoiding oversampling in dense regions close to
the margin and samples all the feature space equivalently \cite{Tuia2009}.
Although, studies employing data level solutions within the remote sensing
domain seem sparse.

In this paper, we test the performance of oversampling techniques on an
imbalanced dataset using different types of classifiers. The effectiveness of
the Geometric SMOTE (G-SMOTE) \cite{Douzas2019} is demonstrated using SMOTE
\cite{Chawla2002}, Borderline SMOTE \cite{Han2005}, ADASYN \cite{HaiboHe2008},
Random Oversampling and no oversampling as baseline methods. Preliminary results
show that G-SMOTE outperforms every other oversampling technique, for both
overall prediction power and minority class prediction power.

Oversamplers' performance is evaluated through an experimental analysis using
the publicly available Lucas Soil dataset \cite{Toth2013}. This dataset contains
multispectral reflectance data from European soil, filtered for a region in
Portugal. The experimental procedure includes a comparison of the different
oversamplers using 4 classifiers and \textbf{(...)} evaluation metrics.

This paper is organized in \textbf{(...)} sections: \textbf{(...)}

\section{State of the Art}

% As I assumed this  exists in the intro, so might be paraphrased or removed
% depending on preferences.
Approaches to class imbalance problem can be classified into three main
categories: Algorithmic level (which includes cost-sensitive methods), data-
level (i.e., data resampling) and hybrid methods (integration of the other two)
\cite{Ji2018}.
% conflicts with the Introduction section (Algorithmic, data level and cost-
% sensitive)

\subsection{algorithm-level methods}
The review of related literature reveals implementation of all three approaches
to handle with imbalance data (class imbalance) problem existing in remote
sensing and Earth observation datasets. A number of publications focus on the
development of such algorithms that force the model to learn towards the
minority classes \cite{Wang2019, Chen2010, Williams2009, Ji2018}.
\cite{Chen2010} uses object-oriented Asymmetrically Local Discriminant
Selection (ALDS) classification method for future extraction from very high
resolution (VHR) imagery. The semi-supervised algorithm uses the spectral,
geometric and textural information of the objects as features, also
incorporates the class size information into weight matrix. \cite{Williams2009}
use Infinitely Imbalanced Logistic Regression (IILR) semi-supervised
classification to detect the mining spots on the Synthetic Aperture Radar (SAR)
imagery while working with naturally imbalanced datasets. This algorithm
explicitly addresses the class imbalance problem and performs the minority
class classification in two steps: minority class border detection and
feature extraction.

\subsection{Data-level methods}
Both generally in data science \cite{Douzas2019} and specifically in
remote sensing \cite{Feng2019}, data modification through resampling methods
are broader and are the most used approaches for imbalanced learning methods.
These approaches balance the data in the preprocessing step. Thus, there is
no need to modify the classifier. Consequently, can be implemented with ease
for multi-class imbalance learning tasks \cite{Feng2019}. Resampling methods
can be further divided into two categories:

\textbf{Undersampling} - Some of the existing literature implements the Random
Undersampling method (RUS), which randomly reduces the number of training
samples belonging to the majority class. However, this method has the
disadvantage of information loss as it discards samples from the majority class
\cite{Feng2019}. \cite{Waske2009} propose the bootstrap aggregating method with
an ensemble of Support Vector Machines (SVM) for performing remotely sensed
high resolution image classification. They use RUS to balance the data between
the majority and minority classes.

% Here another repetition of introduction
\textbf{Oversampling} - Contrary to RUS, the Random Oversampling method (ROS)
benefits from avoiding information loss. However, ROS simply replicates
randomly selected instances of the minority class. Consequently, it carries the 
risk of over-fitting. Regardless, \cite{Feng2019}
considers ROS to be the most popular resampling method in the remote sensing
domain. Thus, this paper uses unmodified data (No Oversampling) and ROS (along
with other commonly used oversamplers) as the baseline to compare different
oversampling methods implemented along with different classifiers. Synthetic
minority oversampling technique (SMOTE) is another famous method in imbalanced
learning which can avoid over-fitting but has the disadvantage of generating
noisy data. There are many variations of SMOTE (such as Borderline-SMOTE,
Majority Weighted Minority Oversampling Technique for Imbalanced Data Set
Learning, ADASYN, KernelADASYN etc.) with the objective of preventing noisy
data generation \cite{Feng2019}. Recently, \cite{Douzas2019}
proposed Geometric-SMOTE oversampling technique, which is the core focus of
this paper. In order to avoid from noisy data generation, Geometric-SMOTE
algorithm defines safe areas around the minority instances. Afterwards, it
expands the minority class area for more informative synthetic sample
generation.

The variational semi-supervised learning (VSSL) proposed in \cite{Cenggoro2018}
aims to fix the class imbalance problem in LULC mapping. VSSL is a semi-
supervised learning framework consisting of a deep generative model which
allows the model to learn both form labeled and unlabeled data. For this
analysis they used SMOTE to balance the data.

% this subsection conflicts with the Introduction
Ensemble learning method together with oversampling algorithms have been
successfully implemented for remote sensing image classification in several
studies. Feng et al. in their research apply Rotation forest (RoF) ensemble
leaning algorithm to handle with class imbalance problem \cite{Feng2019}. Han
et al. \cite{Han2012} implements ensemble algorithm based on Diversity Ensemble
Creation by Oppositional Relabeling of Artificial Training Examples (DECORATEs)
and Random Forest (RF) classifier. This ensemble algorithm uses Radial Basis
Function Neural Network (RBFNN) as a base classifier. Both studies use SMOTE
oversampling method for synthetic data generation.

Ji et al. in their study of detecting collapsed buildings with SqueezeNet method
(a small CNN architecture) from high resolution satellite imagery, they also
carry out a comparison of imbalance learn methods by implementing, namely,
random oversampling, random undersampling, and cost-sensitive methods while
dealing with imbalanced data \cite{Ji2018}. The experiment results show the
cost-sensitive algorithm-level model to outperform ROS and RUS resampling
methods with highest overall accuracy (OA) and Kappa statistics.

In the contrast to the above-discussed examples, \cite{Maxwell2018} reports
that balancing data with RO reduces the overall classification accuracy for
most of the classifiers (DT, ANN, k-NN, Boosted DT) and slightly increases the
overall accuracy for RF and SVM classifiers. The paper shows, that even if for
some minority classes RO can affect positively and on the prediction of
minority classes, however it can corrupt the prediction accuracy of a majority
class.

% Here is also repetition with intro, should be integrated
However, the review of related studies show there is a big lack of researches
comparing the classification accuracy of imbalanced learning methods towards the
minority classes for remote sensing natural imbalanced dataset. This article
carries the study of implementation of different oversampling algorithms
together with Geometric-SMOTE oversampler (which is the first time being applied
for remote sensing imbalance data) in order to reveal the best classification
results for the minority classes for LUCAS dataset. To achieve the goals
k-nearest neighbors (KNN), decision tree (DT) and gradient boosting classifier
(GBC) supervised classifiers have been deployed.

\subsection{Research methodology}

\subsubsection{Lucas data}

% To be written

\subsubsection{Evaluation measures}
Amongst the many evaluation metrics existing for classifier's performance
evaluation, there are several indices that are considered to be preliminary and
fundamental for LULC classification accuracy assessment. Those measures are
classification overall accuracy (OA), user's accuracy or precision (UA) and
producer's accuracy or recall (PA) and are calculated using the classification
confusion matrix \cite{Liu2007}. However, the Lucas soil dataset presents high
inter-class variability. Hence, the minority classes can be difficult to
account for by the classifiers that are designed to maximise the overall
accuracy, as the majority classes are receiving more weight \cite{Inglada2017}.
As it is important to consider class imbalance and classification accuracy in
the minority classes, Overall Accuracy together with Kappa statistics and
F-score metrics will be summarized using the information from the confusion
matrix.

% Formulas to be added once the metrics are selected 
\textbf{Overall Accuracy}
Classification Overall Accuracy is the proportion of the all samples classified
correctly.

\textbf{Kappa statistics}

Kappa statistics (k) measures the proportion of correctly classified units after
considering the probability of chance agreement and is well correlated to area
under the curve of ROC \cite{Freeman2012}. In other words, Kappa coefficient
indicates the agreement between observed agreement and the random agreement,
that might be expected in case of random classifier.

\textbf{F-score}

F-score (or F-measure) is the harmonic mean of UA (fraction of correctly
classified instances with regard to all instances classified as this certain
class in the confusion matrix) and PA (fraction of correctly classified
instances with regard to all instances of that reference class). F-score can be
calculated as for the all classification as well as for each class. The worst
result is at 0 and the best at the value of 1 \cite{Inglada2017}.



\bibliography{references}
\bibliographystyle{apalike}

\end{document}

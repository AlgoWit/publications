\documentclass[parskip=full]{scrartcl}

\pdfoutput=1

\title{Imbalanced Learning in Land Cover Classification  \\ \LARGE{Improving minority class' prediction accuracy using the Geometric SMOTE algorithm}}

\author{
	Georgios Douzas\(^{1}\), Fernando Bacao\(^{1*}\), Jo√£o Fonseca\(^{1}\), Manvel Khudinyan\(^{1}\)
	\\
	\small{\(^{1}\)NOVA Information Management School, Universidade Nova de Lisboa}
	\\
	\small{*Corresponding Author}
	\\
	\\
	\small{Postal Address: NOVA Information Management School, Campus de Campolide, 1070-312 Lisboa, Portugal}
	\\
	\small{Telephone: +351 21 382 8610}
}

\usepackage{breakcites}
\usepackage{float}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{
	a4paper,
	total={170mm,257mm},
	left=18mm,
	right=18mm,
	top=8mm,
}
\usepackage{amsmath}
\newcommand{\inlineeqnum}{\refstepcounter{equation}~~\mbox{(\theequation)}}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.14}
\usepackage{longtable}
\usepackage{tabu}
\usepackage{hyperref}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Classification of imbalanced datasets is a challenging task for standard
algorithms. Although many methods exist to address this problem in different
ways, generating artificial data for the minority class is a more general
approach compared to algorithmic modifications. SMOTE algorithm, as well as any
other oversampling method based on the SMOTE mechanism, generates synthetic
samples along line segments that join minority class instances. In this paper we
propose Geometric SMOTE (G-SMOTE) as a enhancement of the SMOTE data generation
mechanism. G-SMOTE generates synthetic samples in a geometric region of the
input space, around each selected minority instance. While in the basic
configuration this region is a hyper-sphere, G-SMOTE allows its deformation to a
hyper-spheroid. The performance of G-SMOTE is compared against SMOTE as well as
baseline methods. We present empirical results that show a significant
improvement in the quality of the generated data when G-SMOTE is used as an
oversampling algorithm. An implementation of G-SMOTE is made available in the
Python programming language.
\end{abstract}

\section{Introduction}

Accurate Land Use/Land Cover (LULC) classification maps play an important role
in the remote-sensing domain \cite{Cenggoro2018} and are a crucial source of
information for natural resource land managers and forest monitoring programs
\cite{Mellor2015}. This data is being used for a variety of applications,
ranging from environmental monitoring, land change detection, natural hazard
assessment up to agriculture and water/wetland monitoring \cite{Khatami2016}.

% check if this part is 100% correct
While the human eye is based on three color bands (red, green and blue),
spectral imaging divides the vision into many more. The multi/hyper-spectral
context of LULC maps allow the discrimination of pixels among classes with
similar spectral signatures \cite{Marconcini2009}, proving especially useful for
classification tasks. However, datasets that serve this purpose often suffer
from the class imbalanced problem \cite{Feng2019}. In multi-class classification
tasks, this problem is considerably more difficult to address when compared to
binary classification problems, making it one of the most important challenges
in the machine learning research community \cite{Garcia2018}.

In remote-sensing classification problems, the number of targets present is
often very small compared with the remaining classes, especially when using LULC
data \cite{Williams2009, Cenggoro2018}. As an example, in land-mine detection
applications, it is common to have 100 false alarms for each real mine present.

An imbalanced learning problem refers to a skewed distribution of data across
classes in both binary and multi-class classification problems \cite{Abdi2016}.
In this situation, the minority classes contribute less to the minimization of
the objective function, which induces a bias towards the majority class during
the learning process \cite{Douzas2019}. Consequently, as typical classifier
learning methods are designed to work with reasonably balanced datasets, finding
meaningful boundaries becomes a very difficult task \cite{Saez2016}.

% How to justify the usage of the third method? Is it okay this way?
Possible approaches to deal with class imbalance can be divided into three main
groups \cite{Fernandez2013}. 1) Cost-sensitive solutions: adaptations at
algorithmic and/or data level by applying higher misclassification costs for the
examples of the minority class. 2) Algorithmic level solutions: classification
algorithms are adapted or created to reinforce the learning of the positive
class. 3) Data level solutions: A more general approach where class
distributions are rebalanced through the sampling of the data space, thus
diminishing class imbalance. Because this latter method is more general, it is
used for a wide range of applications, thus making it of particular interest.
Regardless, data level solutions carry the disadvantage of yielding an increased
search space \cite{Fernandez2013}.

There are several data level solutions to deal with the imbalanced learning
problem, which can be divided into three groups. Undersampling algorithms reduce
the size of the majority classes, whereas oversampling algorithms attempt to
attribute a higher weight to minority classes by generating artificial data
\cite{Mellor2015}. A hybrid approach, on the other hand, uses both oversampling
and undersampling techniques.

% can't find source for oversampling/undersampling methods frequently used...
% should we include MS-cSV in our experiment, or in this paragraph at all?
% should I include the limitations of each oversampler?
Studies suggest that the usage of data level solutions in remote sensing
problems to balance class distribution seem to outperform the ones models
trained by randomly drawn samples \cite{Wang2019, Mellor2015}.  Studies
employing oversampling methods such as the Synthetic Minority Over-sampling
Technique (SMOTE) \cite{Chawla2002} seem to consistently yield better results
\cite{Johnson2013, Geib2015} compared to no oversampling. Other studies employ
active learning methods such as Margin Sample by Closest Support Vector
(MS-cSV), as it benefits from avoiding oversampling in dense regions close to
the margin and samples all the feature space equivalently \cite{Tuia2009}.
Although, studies employing data level solutions within the remote sensing
domain seem sparse.

In this paper, we test the performance of oversampling techniques on an
imbalanced dataset using different types of classifiers. The effectiveness of
the Geometric SMOTE (G-SMOTE) \cite{Douzas2019} is demonstrated using SMOTE
\cite{Chawla2002}, Borderline SMOTE \cite{Han2005}, ADASYN \cite{HaiboHe2008},
Random Oversampling and no oversampling as baseline methods. Preliminary results
show that G-SMOTE outperforms any other oversampling technique, for both overall
prediction power and minority class prediction power.

Oversamplers' performance is evaluated through an experimental analysis using
the publicly available Lucas Soil dataset \cite{Toth2013}. This dataset contains
multispectral reflectance data from European soil, filtered for a region in
Portugal. The experimental procedure includes a comparison of the different
oversamplers using 4 classifiers and \textbf{(...)} evaluation metrics.

This paper is organized in \textbf{(...)} sections: \textbf{(...)}



\section{State of the Art}
%(as I assumed this  exists in the intro, so might be paraphrased or removed depending on preferences).
In the existing literature there are different approaches to handle with class imbalance problem which can be classified into three main categories: algorithm-level (includes cost-sensitive method), data-level (data resampling) and hybrid methods (integration of the other two) \citep{ji2018identifying}. 

\subsection{algorithm-level methods}
The review of related literature reveals implementation of all three approaches to handle with imbalance data (class imbalance) problem existing in remote sensing and Earth observation datasets. A number of researches focus on the development of such algorithms that force the model to learn towards the minority classes \citep{wang2018imbalanced}, \citep{chen2010semisupervised}, \citep{williams2009mine}, \citep{ji2018identifying}. Chen et al. \citep{chen2010semisupervised} uses object-oriented Asymmetrically Local Discriminant Selection (ALDS) classification method for future extraction from very high resolution (VHR) imagery. The semi-supervised algorithm uses the spectral, geometric and textural information of the objects as features, also incorporates the class size information into weight matrix. Williams et al. \citep{williams2009mine} use Infinitely Imbalanced Logistic Regression (IILR) semi-supervised classification to detect the mining spots on the Synthetic Aperture Radar (SAR) imagery while working with naturally imbalanced datasets. This algorithm explicitly addresses the class imbalance problem and performs the minority class classification in two steps: minority class border detection and feature extraction. 


\subsection{Data-level methods}
As in the data science generally \citep{douzas2019geometric}, also in remote sensing particualrly  \citep{feng2019imbalanced}, the data modification through a resampling methods are more general and are the most used approaches for imbalanced learning methods. Since these approaches balance the data in the preprocessing step and there is no need of modification of learning algorithm, thus these models are easy to be implemented for multiclass imbalance learning tasks  \citep{feng2019imbalanced}. 

\textbf{Downsamplin}.Some of the researches implement undersampling methods by randomly reducing the number of training samples of majority class. However, this method has an disadvantage of loosing important information when discarding samples form the majority class \citep{feng2019imbalanced}.  Weske et al. \citep{waske2009classifying} propose bootstrap aggregating method with an ensemble of Support Vector Machines (SVM) for performing remotely sensed high resolution image classification. In this research they use random undersamling (RUS) to balance the data in between the majority and minority classes.

% here another repetition of intro- baseline estimators
\textbf{Oversamplin}.In the contrast of RUS, random oversampling (ROS) algorithms could avoid from information loss, however it brings a risk of over-fitting as it simply replicates randomly selected instances in of the minority class in the trainig phase \citep{douzas2019geometric}. Still, Feng et al. \citep{feng2019imbalanced} states ROS to be the most popular resampling method used for dealing with multiclass imbalance learning in remote sensing. Thus, this paper uses unmodified data (No Oversampling) and ROS as so-called baseline for comparison of different oversampling methods implemented together with different classifiers. Synthetic minority oversampling technique (SMOTE) is another famous method in imbalanced learning which can avoid from over-fitting but has a disadvantage of generating noisy data. There have been developed many variations of SMOTE (Borderline-SMOTE, Majority Weighted Minority Oversampling Technique for Imbalanced Data Set Learning, ADASYN, KernelADASYN etc.) with the same objective of preventing noisy data generation \citep{douzas2019geometric}, \citep{feng2019imbalanced}. Recently, Douzas and Bacao have developed Geometric-SMOTE oversampling technique which is the core focus of this paper. In order to avoid from noisy data generation, Geometric-SMOTE algorithm defines safe areas around the minority instances. Then it expands the minority class area for more informative synthetic sample generation \citep{douzas2019geometric}. 

Cenggoro et al. implements variational semi-supervised learning (VSSL) algorithm to solve class imbalance problem in remote sensing data for LULC mapping. VSSL is a semi-supervised learning framework for deep generative model which allows the model to learn both form lebelled and unlebelled data. For this analysis they implemented SMOTE oversampler to balance the data \citep{cenggoro2017classification}. 

\subsection{Ensemble Learning}
Ensemble learning together with oversampling algorithms have been successfully implemented for remote sensing image classification in several studies. Feng et al. in their research apply Rotation forest (RoF) ensemble leaning algorithm to handle with class imbalance problem \citep{feng2019imbalanced}. Han et al. \citep{han2012remote} implements ensemble algorithm based on Diversity Ensemble Creation by Oppositional Relabeling of Artificial Training Examples (DECORATEs) and Random Forest (RF) classifier. This ensemble algorithm uses Radial Basis Function Neural Network (RBFNN) as a base classifier. Both studies use SMOTE oversamling method for synthetic data generation.

Ji et al. in their study of detecting collapsed buildings with SqueezeNet method (a small CNN architecture) from high resolution satellite imagery, they also carry out a comparison of imbalance learn methods by implementing, namely, random oversampling, random undersampling, and cost-sensitive methods while dealing with imbalanced data \citep{ji2018identifying}. The experiment results show the cost-sensitive algorithm-level model to outperfrom ROS and RUS resumpling methods with highest overall accuracy (OA) and Kappa statistics.

% here is also repetition with intro, should be intergrated
However, the review of related studies show there is a big lack of studies comparing the classification accuracy of imbalanced learning methods towards the minority classes for remote sensing natural imbalanced datasets. This article carries the study of implementation of different oversampling algorithms together with Geometric-SMOTE oversampler (which is the first time being applied for remote sensing imbalance data) in order to reveal the best classification results for the minority classes for LUCAS dataset.  To acheave the goals k-nearest neighbors (KNN), decision tree (DT) and gradient boosting classifier (GBC) supervised classifiers have been deployed.



\bibliography{references}
\bibliographystyle{apalike}

\end{document}

\documentclass[parskip=full]{scrartcl}

\pdfoutput=1

\title{Imbalanced Learning in Land Cover Classification  \\ \LARGE{Improving minority class' prediction accuracy using the Geometric SMOTE algorithm}}

\author{
	Georgios Douzas\(^{1}\), Fernando Bacao\(^{1*}\), Jo√£o Fonseca\(^{1}\), Manvel Khudinyan\(^{1}\)
	\\
	\small{\(^{1}\)NOVA Information Management School, Universidade Nova de Lisboa}
	\\
	\small{*Corresponding Author}
	\\
	\\
	\small{Postal Address: NOVA Information Management School, Campus de Campolide, 1070-312 Lisboa, Portugal}
	\\
	\small{Telephone: +351 21 382 8610}
}

\usepackage{breakcites}
\usepackage{float}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{
	a4paper,
	total={170mm,257mm},
	left=18mm,
	right=18mm,
	top=8mm,
}
\usepackage{amsmath}
\newcommand{\inlineeqnum}{\refstepcounter{equation}~~\mbox{(\theequation)}}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.14}
\usepackage{longtable}
\usepackage{tabu}
\usepackage{hyperref}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Classification of imbalanced datasets is a challenging task for standard
algorithms. Although many methods exist to address this problem in different
ways, generating artificial data for the minority class is a more general
approach compared to algorithmic modifications. SMOTE algorithm, as well as any
other oversampling method based on the SMOTE mechanism, generates synthetic
samples along line segments that join minority class instances. In this paper we
propose Geometric SMOTE (G-SMOTE) as a enhancement of the SMOTE data generation
mechanism. G-SMOTE generates synthetic samples in a geometric region of the
input space, around each selected minority instance. While in the basic
configuration this region is a hyper-sphere, G-SMOTE allows its deformation to a
hyper-spheroid. The performance of G-SMOTE is compared against SMOTE as well as
baseline methods. We present empirical results that show a significant
improvement in the quality of the generated data when G-SMOTE is used as an
oversampling algorithm. An implementation of G-SMOTE is made available in the
Python programming language.
\end{abstract}

\section{Introduction}

Accurate Land Use/Land Cover (LULC) classification maps play an important role
in the remote-sensing domain \cite{Cenggoro2018} and are a crucial source of
information for natural resource land managers and forest monitoring programs
\cite{Mellor2015}. This data is being used for a variety of applications,
ranging from environmental monitoring, land change detection, natural hazard
assessment up to agriculture and water/wetland monitoring \cite{Khatami2016}.

% Check if this part
While the human eye is based on three color bands (red, green and blue),
spectral imaging divides the vision into many more. The multi/hyper-spectral
context of LULC maps allow the discrimination of pixels among classes with
similar spectral signatures \cite{Marconcini2009}, proving especially useful for
classification tasks. However, datasets that serve this purpose often suffer
from the class imbalanced problem \cite{Feng2019}. In multi-class classification
tasks, this problem is considerably more difficult to address when compared to
binary classification problems, making it one of the most important challenges
in the machine learning research community \cite{Garcia2018}.

In remote-sensing classification problems, the number of targets present is
often very small compared with the remaining classes, especially when using LULC
data \cite{Williams2009, Cenggoro2018}. As an example, in land-mine detection
applications, it is common to have 100 false alarms for each real mine present.

An imbalanced learning problem refers to a skewed distribution of data across
classes in both binary and multi-class classification problems \cite{Abdi2016}.
In this situation, the minority classes contribute less to the minimization of
the objective function, which induces a bias towards the majority class during
the learning process \cite{Douzas2019}. Consequently, as typical classifier
learning methods are designed to work with reasonably balanced datasets, finding
meaningful boundaries becomes a very difficult task \cite{Saez2016}.

% How to justify the usage of the third method? Is it okay this way?
Possible approaches to deal with class imbalance can be divided into three main
groups \cite{Fernandez2013}. 1) Cost-sensitive solutions: adaptations at
algorithmic and/or data level by applying higher misclassification costs for the
examples of the minority class. 2) Algorithmic level solutions: classification
algorithms are adapted or created to reinforce the learning of the positive
class. 3) Data level solutions: A more general approach where class
distributions are rebalanced through the sampling of the data space, thus
diminishing class imbalance. Because this latter method is more general, it is
used for a wide range of applications, thus making it of particular interest.
Regardless, data level solutions carry the disadvantage of yielding an increased
search space \cite{Fernandez2013}.

There are several data level solutions to deal with the imbalanced learning
problem, which can be divided into three groups. Undersampling algorithms reduce
the size of the majority classes, whereas oversampling algorithms attempt to
attribute a higher weight to minority classes by generating artificial data
\cite{Mellor2015}. A hybrid approach, on the other hand, uses both oversampling
and undersampling techniques.

% Can't find source for oversampling/undersampling methods frequently used...
% Should we include MS-cSV in our experiment, or in this paragraph at all?
% Should I include the limitations of each oversampler?
Studies suggest that the usage of data level solutions in remote sensing
problems to balance class distribution seem to outperform the ones models
trained by randomly drawn samples \cite{Wang2019, Mellor2015}.  Studies
employing oversampling methods such as the Synthetic Minority Over-sampling
Technique (SMOTE) \cite{Chawla2002} seem to consistently yield better results
\cite{Johnson2013, Geib2015} compared to no oversampling. Other studies employ
active learning methods such as Margin Sample by Closest Support Vector
(MS-cSV), as it benefits from avoiding oversampling in dense regions close to
the margin and samples all the feature space equivalently \cite{Tuia2009}.
Although, studies employing data level solutions within the remote sensing
domain seem sparse.

In this paper, we test the performance of oversampling techniques on an
imbalanced dataset using different types of classifiers. The effectiveness of
the Geometric SMOTE (G-SMOTE) \cite{Douzas2019} is demonstrated using SMOTE
\cite{Chawla2002}, Borderline SMOTE \cite{Han2005}, ADASYN \cite{HaiboHe2008},
Random Oversampling and no oversampling as baseline methods. Preliminary results
show that G-SMOTE outperforms any other oversampling technique, for both overall
prediction power and minority class prediction power.

Oversamplers' performance is evaluated through an experimental analysis using
the publicly available Lucas Soil dataset \cite{Toth2013}. This dataset contains
multispectral reflectance data from European soil, filtered for a region in
Portugal. The experimental procedure includes a comparison of the different
oversamplers using 4 classifiers and \textbf{(...)} evaluation metrics.

This paper is organized in \textbf{(...)} sections: \textbf{(...)}

\section{State of the Art}

% As I assumed this  exists in the intro, so might be paraphrased or removed
% depending on preferences.
In the existing literature there are different approaches to handle with class
imbalance problem which can be classified into three main categories:
algorithm-level (includes cost-sensitive method), data-level (data resampling)
and hybrid methods (integration of the other two) \cite{Ji2018}. 

\subsection{algorithm-level methods}
The review of related literature reveals implementation of all three approaches
to handle with imbalance data (class imbalance) problem existing in remote
sensing and Earth observation datasets. A number of researches focus on the
development of such algorithms that force the model to learn towards the
minority classes \cite{Wang2018}, \cite{Chen2010}, \cite{Williams2009},
\cite{Ji2018}. Chen et al. \cite{Chen2010} uses object-oriented Asymmetrically
Local Discriminant Selection (ALDS) classification method for future extraction
from very high resolution (VHR) imagery. The semi-supervised algorithm uses the
spectral, geometric and textural information of the objects as features, also
incorporates the class size information into weight matrix. Williams et al.
\cite{Williams2009} use Infinitely Imbalanced Logistic Regression (IILR)
semi-supervised classification to detect the mining spots on the Synthetic
Aperture Radar (SAR) imagery while working with naturally imbalanced datasets.
This algorithm explicitly addresses the class imbalance problem and performs the
minority class classification in two steps: minority class border detection and
feature extraction. 

\subsection{Data-level methods}
As in the data science generally \cite{Douzas2019}, also in remote sensing
particularly \cite{Feng2019}, the data modification through a resampling methods
are more general and are the most used approaches for imbalanced learning
methods. Since these approaches balance the data in the preprocessing step and
there is no need of modification of learning algorithm, thus these models are
easy to be implemented for multi-class imbalance learning tasks \cite{Feng2019}.


\textbf{Undersampling}.Some of the researches implement undersampling methods by
randomly reducing the number of training samples of majority class. However,
this method has an disadvantage of loosing important information when discarding
samples form the majority class \cite{Feng2019}. \cite{Waske2009} propose
bootstrap aggregating method with an ensemble of Support Vector Machines (SVM)
for performing remotely sensed high resolution image classification. In this
research they use random undersampling (RUS) to balance the data in between the
majority and minority classes.

% Here another repetition of introduction
\textbf{Oversampling}.In the contrast of RUS, random oversampling (ROS)
algorithms could avoid from information loss, however it brings a risk of
over-fitting as it simply replicates randomly selected instances in of the
minority class in the training phase \cite{Douzas2019}. Still, \cite{Feng2019}
states ROS to be the most popular resampling method used for dealing with
multi-class imbalance learning in remote sensing. Thus, this paper uses
unmodified data (No Oversampling) and ROS as so-called baseline for comparison
of different oversampling methods implemented together with different
classifiers. Synthetic minority oversampling technique (SMOTE) is another famous
method in imbalanced learning which can avoid from over-fitting but has a
disadvantage of generating noisy data. There have been developed many variations
of SMOTE (Borderline-SMOTE, Majority Weighted Minority Oversampling Technique
for Imbalanced Data Set Learning, ADASYN, KernelADASYN etc.) with the same
objective of preventing noisy data generation \cite{Douzas2019},
\cite{Feng2019}. Recently, Douzas and Bacao have developed Geometric-SMOTE
oversampling technique which is the core focus of this paper. In order to avoid
from noisy data generation, Geometric-SMOTE algorithm defines safe areas around
the minority instances. Then it expands the minority class area for more
informative synthetic sample generation \cite{Douzas2019}. 

Cenggoro et al. implements variational semi-supervised learning (VSSL) algorithm
to solve class imbalance problem in remote sensing data for LULC mapping. VSSL
is a semi-supervised learning framework for deep generative model which allows
the model to learn both form labelled and unlabelled data. For this analysis
they implemented SMOTE oversampler to balance the data \cite{Cenggoro2017}. 

\subsection{Ensemble Learning}
Ensemble learning together with oversampling algorithms have been successfully
implemented for remote sensing image classification in several studies. Feng et
al. in their research apply Rotation forest (RoF) ensemble leaning algorithm to
handle with class imbalance problem \cite{Feng2019}. Han et al. \cite{Han2012}
implements ensemble algorithm based on Diversity Ensemble Creation by
Oppositional Relabeling of Artificial Training Examples (DECORATEs) and Random
Forest (RF) classifier. This ensemble algorithm uses Radial Basis Function
Neural Network (RBFNN) as a base classifier. Both studies use SMOTE oversampling
method for synthetic data generation.

Ji et al. in their study of detecting collapsed buildings with SqueezeNet method
(a small CNN architecture) from high resolution satellite imagery, they also
carry out a comparison of imbalance learn methods by implementing, namely,
random oversampling, random undersampling, and cost-sensitive methods while
dealing with imbalanced data \cite{Ji2018}. The experiment results show the
cost-sensitive algorithm-level model to outperform ROS and RUS resampling
methods with highest overall accuracy (OA) and Kappa statistics.

% Here is also repetition with intro, should be integrated
However, the review of related studies show there is a big lack of researches
comparing the classification accuracy of imbalanced learning methods towards the
minority classes for remote sensing natural imbalanced dataset. This article
carries the study of implementation of different oversampling algorithms
together with Geometric-SMOTE oversampler (which is the first time being applied
for remote sensing imbalance data) in order to reveal the best classification
results for the minority classes for LUCAS dataset.  To achieve the goals
k-nearest neighbors (KNN), decision tree (DT) and gradient boosting classifier
(GBC) supervised classifiers have been deployed.

\subsection{Research methodology}

\subsubsection{Lucas data}

% To be written

\subsubsection{Evaluation measures}
Amongst the many evaluation metrics existing for classifier's performance
evaluation, there are several measures that are considered to be preliminary and
fundamental for LULC classification accuracy assessment with remotely sensed
dataset. Those measures are classification overall accuracy (OA), user's
accuracy (UA) and producer's accuracy (PA) and are calculated form
classification confusion matrix \cite{Liu2007}. Together with those basic ones
another very popular and widely used for remote sensing imbalanced data
classification measure is Kappa statistics \cite{Du2015}, \cite{Wang2018},
\cite{Foody2002}. Producer's Kappa and area under the curve (AUC) are being used
also for multi-class imbalanced dataset in order to evaluate classifier's
performance for each class \cite{Freeman2012}, \cite{Mellor2015}.

\bibliography{references}
\bibliographystyle{apalike}

\end{document}

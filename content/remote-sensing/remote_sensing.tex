\documentclass[parskip=full]{scrartcl}

\pdfoutput=1

\title{Imbalanced Learning in Land Cover Classification  \\ \LARGE{Improving minority class' prediction accuracy using the Geometric SMOTE algorithm}}

\author{
	Georgios Douzas\(^{1}\), Fernando Bacao\(^{1*}\), Joao Fonseca\(^{1}\), Manvel Khudinyan\(^{1}\)
	\\
	\small{\(^{1}\)NOVA Information Management School, Universidade Nova de Lisboa}
	\\
	\small{*Corresponding Author}
	\\
	\\
	\small{Postal Address: NOVA Information Management School, Campus de Campolide, 1070-312 Lisboa, Portugal}
	\\
	\small{Telephone: +351 21 382 8610}
}

\usepackage{breakcites}
\usepackage{float}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{geometry}
\geometry{
	a4paper,
	total={170mm,257mm},
	left=18mm,
	right=18mm,
	top=8mm,
}
\usepackage{amsmath}
\newcommand{\inlineeqnum}{\refstepcounter{equation}~~\mbox{(\theequation)}}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.14}
\usepackage{longtable}
\usepackage{tabu}
\usepackage{hyperref}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Abstract goes here
\end{abstract}

\section{Introduction}

The production of accurate Land Use/Land Cover maps offer unique monitoring
capabilities within the remote-sensing domain \cite{Mellor2015}. LULC maps are
being used for a variety of applications, ranging from environmental
monitoring, land change detection, natural hazard assessment up to agriculture
and water/wetland monitoring \cite{Khatami2016}.

Multispectral images are an important resource to build LULC maps, allowing for
the use of classification algorithms, both supervised and unsupervised, to
automate the production of these maps. Although significant progress has been
made in the use of supervised learning techniques for automatic image
classification \cite{Tewkesbury2015}, the acquisition of labeled training sets
continues to be a bottleneck \cite{Rajan2008}. To build  good and robust
supervised classifiers it is crucial to have a large enough training dataset.
Often, the problem is that different land use classes have very different
levels of coverage, while some are frequent in the training dataset, other can
be limited \cite{Feng2019}. In remote-sensing classification problems, the
number of instances of some classes is often very small when compared with the
dominant classes, this is especially true in LULC data \cite{Williams2009,
Cenggoro2018}.

This imbalance in the representation of the different classes constitutes a
problem for the development of accurate classifiers. This problem is usually
referred to, in the machine learning community, as the imbalanced learning
problem \cite{Chawla2004}. The imbalanced learning problem is even more
difficult to address in multi-class problems, making it one of the most
important challenges in the machine learning research community
\cite{Garcia2018}. Improvements in the representation of small classes in
training datasets can have a significant impact in improving the accuracy and
robustness of classifiers for the production of LULC maps.

An imbalanced learning problem refers to a skewed distribution of data across
classes in both binary and multi-class classification problems \cite{Abdi2016}.
In this situation, the minority classes contribute less to the minimization of
accuracy, the typical objective function, inducing a bias towards the majority
class during the learning process \cite{Douzas2019}. Consequently, as typical
classifier learning methods are designed to work with reasonably balanced
datasets, finding meaningful boundaries becomes a very difficult task
\cite{Saez2016}.

Possible approaches to deal with class imbalance can be divided into three main
groups \cite{Fernandez2013}. 1) Cost-sensitive solutions: adaptations at
algorithmic and/or data level by applying higher misclassification costs for the
examples of the minority class. 2) Algorithmic level solutions: classification
algorithms are adapted or created to reinforce the learning of the positive
class. 3) Data level solutions: A more general approach where class
distributions are rebalanced through the sampling of the data space, thus
diminishing class imbalance. Because this latter method is more general, it is
used for a wide range of applications, thus making it of particular interest.
Regardless, data level solutions carry the disadvantage of yielding an increased
search space.

There are several data level solutions to deal with the imbalanced learning
problem, which can be divided into three groups. While Undersampling algorithms
reduce the size of the majority classes, oversampling algorithms attempt to
even the distributions by generating artificial data for the minority class
\cite{Mellor2015}. A hybrid approach, on the other hand, uses both oversampling
and undersampling techniques.

Studies suggest that the usage of data level solutions in remote sensing
problems to balance class distribution seem to outperform models
trained by randomly drawn samples \cite{Wang2019, Mellor2015}.  Studies
employing oversampling methods such as the Synthetic Minority Over-sampling
Technique (SMOTE) \cite{Chawla2002} seem to consistently yield better results
\cite{Johnson2013, Geib2015} compared to no oversampling. Other studies employ
active learning methods such as Margin Sample by Closest Support Vector
(MS-cSV), as it benefits from avoiding oversampling in dense regions close to
the margin and samples all the feature space equivalently \cite{Tuia2009}.
Although, studies employing data level solutions within the remote sensing
domain seem sparse.

In this paper, we test the performance of oversampling techniques on an
imbalanced dataset using different types of classifiers. The effectiveness of
the Geometric SMOTE (G-SMOTE) \cite{Douzas2019} is demonstrated using SMOTE
\cite{Chawla2002}, Borderline SMOTE \cite{Han2005}, ADASYN \cite{HaiboHe2008},
Random Oversampling and no oversampling as baseline methods. Preliminary results
show that G-SMOTE outperforms every other oversampling technique, for both
overall prediction power and minority class prediction power.

Oversamplers' performance is evaluated through an experimental analysis using
the publicly available LUCAS dataset \cite{Toth2013}. This dataset contains
multispectral reflectance data from European soil, filtered for a region in
Portugal. The experimental procedure includes a comparison of the different
oversamplers using 4 classifiers and \textbf{(...)} evaluation metrics.

This paper is organized in \textbf{(...)} sections: \textbf{(...)}

\section{State of the Art}

Generally in machine learning \cite{Douzas2019} and, particularly, in
remote sensing \cite{Feng2019}, data modification through resampling methods
are broader and the most used approaches to deal with the imbalanced learning
problem. These approaches balance the data in the preprocessing step, thus
eliminating the need for model modification and allowing the user to apply any
standard algorithm. Additionally, resampling methods can be easily adapted to a
multi-class imbalanced data, which is a relevant one for LULC classification
case \cite{Feng2019}. In this section the most relevant studies with the
implementation of resampling methods for remote sensing imbalanced data
classification are presented. The oversampling methods are pointed out.

Some of the existing studies implements the random undersampling method,
which randomly reduces the number of training samples  belonging to the
majority class. However, this method has the disadvantage of information loss
as it discards samples from the majority class  \cite{Feng2019}.
\cite{Feng2018} caries out competitive analysis of  undersamplers and
oversamplers performance for land cover classification with  Rotation Forest
(RoF) ensemble classifier. For the experiment random  undersampling,
undersampling based RoF and number of oversampling techniques  were deployed.
The experiment dataset was 16 class imbalanced data from  hypercritical
imagery. The result analysis reveals the low prediction power of the
undersampling methods as they have always been the lowest for the prediction of
all 16 classes.

Contrary to random undersampling, the random oversampling method benefits from
avoiding information loss. However, this method simply replicates randomly
selected instances of the minority class, consequently, increasing the risk of
over-fitting. \cite{Maxwell2018} reports that balancing data with random
oversampling affects the classification performance differently for different
supervised models. In their paper land cover classification with highly
imbalanced data is carried out with six different supervised classifiers.
Implementation of random oversampling could slightly improve the performance of
Random Forest (RF) and Support Vector Machine (SVM) classifiers. On the other
hand it reduced the overall classification accuracy for classifiers such as
Decision Tree (DT), Artificial Neural Network (ANN), k-Nearest Neighbors (k-NN)
and Boosted DT. The paper shows, that even if random oversampling can affect
positively on the prediction of minority classes, however it can corrupt the
prediction accuracy of a majority class.

Synthetic minority oversampling technique (SMOTE) appears in the resent studies
to be one of the most applied oversampling method to successfully handle with
class imbalance problem in land cover classification with imbalanced remote
sensing data. The variational semi-supervised learning (VSSL) proposed in
\cite{Cenggoro2018} aims to fix the class imbalance problem in LULC mapping.
VSSL is a semi-supervised learning framework consisting of a deep generative
model which allows the model to learn both form labeled and unlabeled data. For
this analysis they used SMOTE technique to balance the data. The result shows
significant performance gain on the producer's accuracy of minority class
prediction compared to the results without any imbalanced learning.

Ensemble learning method together with oversampling algorithms have been
successfully implemented for remote sensing image classification in the resent
years. \cite{Feng2018}, \cite{Feng2019} in their research apply Rootation
Forest ensemble learning algorithm with SMOTE to handle with class imbalance
problem. These papers offer a novel approach of SMOTE based RoF algorithm
for a hyperspectral image classification. The idea behind is to iteratively
balance the class distribution with SMOTE before creating each rotation
decision tree. The experiments results yield about the improved classification
for the minority classes as well as for the whole dataset after balancing data
by oversampling.

A number of studies report of notable improvement in LULC mapping accuracy
while applying SMOTE oversampling with standard algorithms to handle with data
imbalance problem for multi-source remote sensing data. \cite{Johnson2016}
uses OpenStreetMap crowdsourced data and Landsat time series for LULC
classification. Simple implementation of SMOTE oversampler shows improved
classification results for all the supervised classifiers (naive bayes, DT and
RF). \cite{Bogner2018} use RF classifier to detect rare land use classes on
MODIS time series. They use SMOTE oversampler to decrease the imbalance ratio
in the original dataset. The comparison of different classification scenarios
shows the minority classes to be better classified after generating synthetic
instances on the training dataset using SMOTE oversampling. However, the study
reports that non of the minority classes could record as high classification
accuracy as the ones for majority classes. \cite{Panda2018} carry out a simple
implementation of SMOTE with five different classifiers (Nave Bayes, DT, RF,
k-NN, SVM) to predict a land cover maps with 4, 5 and 6 classes. The
experimental output highlights the dominance of prediction power of balanced
data over the original imbalanced data. All 5 classifiers show improved
classification accuracy after applying SMOTE oversampling.

Even if the resent studies demonstrate the usefulness of oversampling methods
for remote sensing applications, they are still having drawbacks. Random
oversampling often leads to the model over-fitting as it simply replicates the
instances for minority class \cite{Feng2019}. SMOTE method can avoid
over-fitting but has the disadvantage of generating noisy data \cite{He2008}.
In order to mitigate this problem many variations of SMOTE has been developed
with the objective of preventing noisy data generation (see section 1).

In this study we demonstrate the performance of recently proposed
Geometric-SMOTE oversampling technique developed by \cite{Douzas2019}. The
method appears to overcome the above mentioned problem and increase the LULC
classification accuracy with LUCAS imbalanced dataset. In order to avoid from
noisy data generation, Geometric-SMOTE algorithm defines safe areas around the
minority instances. Afterwards, it expands the minority class area for more
informative synthetic sample generation. In this work the comparative analysis
of different oversampling methods are demonstrated for classification of highly
imbalanced LUCAS dataset. To achieve the goals KNN, DT and Gradient Boosting
classifier (GBC) supervised classifiers have been deployed.

\section{Methodology}

This section explains the evaluation process of G-SMOTE's performance. A
description of the baseline methods, classification algorithms, evaluation
methods, dataset used and experimental procedure is provided.

The implementation of the experimental procedure was based on the Python
programming language, using the Scikit-Learn \cite{Pedregosa2011}, Imbalanced-
Learn \cite{JMLR:v18:16-365} and Geometric-SMOTE \cite{Douzas2019} libraries.
The experiments reported in this paper as well the analysis of the results are
reproducible using the scripts available at \url{https://github.com/AlgoWit/publications}.

\subsection{Baseline methods}

Four oversampling techniques were used in this experiment as baseline methods
along with Geometric SMOTE. Random oversampling was chosen for its simplicity.
SMOTE was selected for being a classic technique and the most popular
oversampling algorithm \cite{Douzas2019}. ADASYN \cite{HaiboHe2008} and
Borderline SMOTE \cite{Han2005} were selected for representing popular
modifications of the original SMOTE technique in the selection phase and data
generation mechanism, respectively. Finally, we use no oversampling as an
additional baseline method.

\subsection{Classification algorithms}

The selection of classification algorithms was done according to 3 criteria:
learning type, training time and popularity within the remote sensing
community. We further divide the relevant algorithms into 4 different learning
types: neighbours-based, rule-based, ensemble and generalised linear methods.

\todo{maybe move this part to the experimental setup?}
For each combination of oversampler and classifier (represented as $x_a$ and
$y_b$, respectively), a grid search will be run in order to find the optimal
parameter settings. The number of fits for a single combination  is defined as:

\[
fits_{x_a,y_b}=\prod\limits_{p=1} \textrm{count}(x_a^p).\prod\limits_{q=1}
\textrm{count}(y_b^q)
\]

Where $\textrm{count}(x_a^p)$ corresponds to the search grid size of
hyperparameter $p$ of an oversampler $x_a$. Finally, the total number of fits
is defined as:

\[
Total\ Fits=k\sum\limits_{a=1} \sum\limits_{b=1} fits_{x_a,y_b}
\]

Where $k$ is the number of folds defined for cross-validation. Thus, adding new
classifiers, oversamplers or grid search values may exponentially increase the
time necessary to run the experiment. So, in order to setup an extensive grid
search and run the full experiment within feasible time, selecting
computationally efficient classifiers becomes vital.

\cite{Khatami2016} developed a meta-analysis of classification processes out
of 266 manuscripts published from 1998 to 2012. Additionally,
\cite{Maxwell2018} also describe mature classification methods used in remote
sensing. The most common classifiers in supervised pixel-based land-cover image
classification processes include K-nearest neighbours, Support Vector Machines,
Decision Trees, Boosted Decision Trees, Random Forests and Neural Networks.
None of the manuscripts mention the use of generalised linear models.

Amongst the neighbours-based learning methods the K-nearest neighbours (KNN)
algorithm was selected. Out of the rule-based learning methods, the decision
tree classifier (DT) was chosen. Two ensemble learning methods were chosen:
Gradient boosting classifier and Random Forest classifier. Finally, the
logistic regression classifier was selected amongst the generalised linear
models, since it is a commonly used baseline algorithm. All these algorithms
were found to be computationally efficient and commonly used classifiers for
the proposed task (with exception for the logistic regression).

Despite their common use, both artificial neural networks and Support
Vector Machines were not included in this experiment due to their long training
times.

\subsection{Evaluation methods}

Amongst the many evaluation metrics existing for classifier's performance
evaluation, there are several indices that are considered to be preliminary and
are widely used for LULC classification accuracy assessment. Those measures are
classification overall accuracy, user's accuracy or precision and
producer's accuracy or recall,  and are calculated using the classification
confusion matrix \cite{Liu2007}. However, the LUCAS dataset has high
inter-class variability. Hence, the minority classes can be difficult to
account for by the classifiers that are designed to maximise the overall
accuracy, as the majority classes are receiving more weight \cite{Inglada2017}.
On the other hand, \cite{He2008} states that the use of user's and producer's
accuracy for class imbalanced data is not straightforward and producer's
accuracy is not sensitive to the data distribution at all. Considering these,
class imbalance specific metrics are needed to better describe classifier's
performance. For this purpose, F-score and G-mean metrics are used to evaluate
and compare the performance of the oversampling methods. Classification overall
accuracy (OA) is provided to summarize the confusion matrix.

\subsubsection{Overall Accuracy}

Classification Overall Accuracy is the number of correctly classified samples
divided by the sum of all the samples on the confusion matrix.

\subsubsection{F-score}

F-score (or F-measure) is the harmonic mean of user's accuracy (precision) and
producer's accuracy (recall), where user's accuracy is the fraction of
correctly classified instances with regard to all instances classified as this
certain class in the confusion matrix, and producer's accuracy is the fraction
of  correctly classified instances with regard to all instances of that
reference class. The F-score value for multi-class imbalanced data
classification can be calculated considering the user's accuracy and producer's
accuracy as a unweighted average of the respective values from all the classes.

\[
F{-}score=2\frac{\text{E(UA)} \times \text{E(PA)}}{\text{E(UA)} + \text{E(PA)}}
\]

Where E(UA) and E(PA) are unweighted mean values of user's and producer's
accuracy of all the classes, respectively.

\subsubsection{G-measure}

The geometric mean score (G-mean) is the root of the product of class-wise
sensitivity and specificity. For the multi-class case sensitivity and
specificity are calculated for each class, then averaged with simple mean
value. Here the sensitivity (true positive rate) is the same with recall
value (producer's accuracy), and specificity (true negative rate) is defines as
the proportion of actual negatives classified as such.

\[
G{-}mean = \sqrt{E(Sensitivity) \times E(Specificity)}
\]

Where E(Sensitivity) and E(Specificity) are the unweighted mean values of
sensitivity and specificity of all the classes, respectively.


\subsection{Experiment Settings}

As it is stated in the section 3.2, machine-learning classifiers usually have
parameters that need to be set up by users for a better classification
performance. Parameter tuning to define the optimal values of the
hyper-parameters was performed with 5-fold cross-validation (CV) for each of
the model (combining classifiers with oversampler). The best parameters are
later selected to validate the classifie'r performance.

In order to avoid from over-optimistic classification results, implementation
of CV with oversampling technique should be carried out as following
\cite{Lusa2015}. In $k$-fold CV dataset is divided into $k$ parts, one part is
excluded and used as only test set, while the rest $k$-1 part is used to train
the model. This process is iterated and each of the $k$ folds are used only
once as a test set to evaluate the classifier's performance. Later, results are
averaged across all $k$ iterations. It is crucial to remember that
all the steps for building the prediction model should be applied only on the
training data. From this respect, the oversampling (or any kind of data
resampling) technique should not be applied on the whole dataset. Instead, it
is used only on the training dataset generated in each step of the CV
procedure.

A range of hyper-parameters used for the grid search for each of the
classifier and oversampler are presented in the table \ref{tab:datasets}.

\begin{table}[H]
	\centering
	\begin{tabular}{lll}
		\toprule
		Classifier       & Hyper-Parameters & Values\\
		\toprule
		LR               & maximum iterations   & 1e4   \\
		KNN              & number of neighbors  & {3, 5} \\
		DT               & maximum depth        & {3, 6} \\
		GBC              & maximum depth        & {3, 6} \\
    			 		 & number of estimators & {50, 100} \\
		RF               & maximum depth        &  {None, 3, 6} \\
				 		 & number of estimators & {50, 100} \\
		\toprule
		Oversampler      &                      & \\
		\toprule
		G-SMOTE          & number of neighbors  & {3, 5} \\
				 		 & selection strategy   & combined, minority, majority\\
				 		 & truncation factor    & {-1.0, -0.5, .0, 0.25,
				 		 0.5, 0.75, 1.0} \\
				 		 & deformation factor   & {0, 0.2, 0.4, 0.5,
				 		 0.6, 0.8, 1.0} \\
 		SMOTE            & number of neighbors & {3, 5} \\
		BORDERLINE SMOTE & number of neighbors & {3, 5} \\
		ADASYN           & number of neighbors & {2, 3} \\
		\bottomrule
	\end{tabular}
	\caption{\label{tab:datasets}User-defined grid values}
\end{table}


\subsection{Data Set}

For this research LUCAS dataset and Landsat 8 time series from 2015 were used
for training and validation of supervised classifiers. 1694 LUCAS points are
used as reference ground-truth data. This dataset is grouped in 8 classes and
represents the land cover types for the study area. The area of study is the
central part of Portugal. The dataset's class distribution and imbalance ratio
is presented in Table \ref{tab:dataset_classes}. The data imbalance becomes
visible with a minority class of 4 instances (wetlands) and majority class of
761 instances (woodland).

\begin{table}[H]
	\centering
	\begin{tabular}{llrrrr}
		\toprule
		\textbf{LUCAS Category} & \textbf{Land cover type} & \textbf{Instances}
		& \textbf{Imbalance Ratio} \\
		\hline
		A & Artificial land & 131 & 5.81 \\
		B & Cropland        & 270 & 2.81 \\
		C & Woodland        & 761 & 1.00 \\
		D & Shrubland       & 296 & 2.61 \\
		E & Grassland       & 185 & 4.11 \\
		F & Bareland        & 37  & 20.56 \\
		G & Water           & 10  & 76.10 \\
		H & Wetlands        & 4   & 190.25\\
		\hline
		\textbf{Total} & & \textbf{1694} &  \textbf{190.25} \\
		\bottomrule
	\end{tabular}
	\caption{\label{tab:dataset_classes}LUCAS nomenclature and sample distribution}
\end{table}

The remote sensing dataset presents 8 images from Landsat 8ETM+ multi-spectral
sensor. The images are Level-1 terrain corrected products (L1TP) and are
acquired in 2015 form February to September, 1 image each month. Only bands 2,
3, 4, 5, 6 and 7 are used from each image. Thus, each reference point from
LUCAS dataset has 48 features (Table \ref{tab:datasets}). Those features have
the value of digital numbers of the images representing LUCAS point location.


\begin{table}[H]
	\centering
	\begin{tabular}{ll}
		\toprule
		\textbf{Dataset name}             &  				 LUCAS \\
		\textbf{Features}                 &             48 \\
		\textbf{Instances}                &           1694 \\
		\textbf{Minority class instaces}  &              4 \\
		\textbf{Majority class instances} &            761 \\
		\textbf{Imbalance Ratio}          &         190.25 \\
	\bottomrule
	\end{tabular}
	\caption{\label{tab:datasets}Description of the dataset}
\end{table}

\section{Results and Discussion}

\begin{table}[H]
	\centering
	\begin{tabular}{llrrrrrr}
	\toprule
	Classifier &        Metric &  Random OS &  G-SMOTE &  B-SMOTE &   No OS &   SMOTE &  ADASYN \\
	\midrule
	        LR &  G-mean macro &     0.5287 &   0.5657 &   0.5300 &  0.5134 &
					0.5245 &  0.5176 \\
	        LR &      F1 macro &     0.2927 &   0.3125 &   0.2995 &  0.2956 &
					0.2881 &  0.2816 \\
	        LR &      Accuracy &     0.4990 &   0.5057 &   0.5321 &  0.5744 &
					0.4953 &  0.4801 \\
					\hline
	       KNN &  G-mean macro &     0.4776 &   0.5037 &   0.5005 &  0.4962 &
				 0.4866 &  0.4829 \\
	       KNN &      F1 macro &     0.2425 &   0.2795 &   0.2633 &  0.2741 &
				 0.2475 &  0.2438 \\
	       KNN &      Accuracy &     0.4459 &   0.5573 &   0.4911 &  0.5579 &
				 0.4256 &  0.4193 \\
				 \hline
	        DT &  G-mean macro &     0.4832 &   0.5194 &   0.5082 &  0.4883 &
					0.4899 &  0.4921 \\
	        DT &      F1 macro &     0.2427 &   0.2674 &   0.2715 &  0.2431 &
					0.2500 &  0.2500 \\
	        DT &      Accuracy &     0.4313 &   0.4786 &   0.4742 &  0.5142 &
					0.4189 &  0.4166 \\
					\hline
	       GBC &  G-mean macro &     0.5373 &   0.5592 &   0.5453 &  0.5322 &
				 0.5403 &  0.5375 \\
	       GBC &      F1 macro &     0.3099 &   0.3293 &   0.3145 &  0.3129 &
				 0.3128 &  0.3062 \\
	       GBC &      Accuracy &     0.5602 &   0.5736 &   0.5657 &  0.5838 &
				 0.5598 &  0.5514 \\
				 \hline
	        RF &  G-mean macro &     0.5418 &   0.5723 &   0.5496 &  0.5282 &
					0.5452 &  0.5422 \\
	        RF &      F1 macro &     0.3135 &   0.3413 &   0.3147 &  0.3057 &
					0.3172 &  0.3141 \\
	        RF &      Accuracy &     0.5763 &   0.5795 &   0.5708 &  0.5874 &
					0.5571 &  0.5519 \\
	\bottomrule
	\end{tabular}
	\caption{\label{tab:scores}Scoring table}
\end{table}

\begin{table}[H]
	\centering
	\begin{tabular}{lrrr}
	\toprule
	{} &  F1 macro &  G-mean macro &  Accuracy \\
	\midrule
	RANDOM OVERSAMPLING &       5.2 &           5.0 &       3.8 \\
	G-SMOTE             &       1.2 &           1.0 &       2.2 \\
	BORDERLINE SMOTE    &       2.2 &           2.0 &       3.0 \\
	NO OVERSAMPLING     &       3.8 &           5.2 &       1.0 \\
	SMOTE               &       3.8 &           3.6 &       5.0 \\
	ADASYN              &       4.8 &           4.2 &       6.0 \\
	\bottomrule
	\end{tabular}
	\caption{\label{tab:rankings}Ranking table}
\end{table}


\section{Conclusions}



\bibliography{references}
\bibliographystyle{apalike}

\end{document}

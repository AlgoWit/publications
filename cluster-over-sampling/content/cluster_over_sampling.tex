\documentclass[parskip=full]{scrartcl}

\pdfoutput=1

\title{Comparing the performance of oversampling techniques in combination with a clustering algorithm for imbalanced learning}

\author{
	Georgios Douzas\(^{*1}\), Fernando Bacao\(^{1}\)
	\\
	\small{\(^{1}\)NOVA Information Management School, Universidade Nova de Lisboa}
	\\
	\small{*Corresponding Author}
	\\
	\\
	\small{Postal Address: NOVA Information Management School, Campus de Campolide, 1070-312 Lisboa, Portugal}
	\\
	\small{Telephone: +351 21 382 8610}
}

\usepackage{breakcites}
\usepackage{float}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{
	a4paper,
	total={170mm,257mm},
	left=18mm,
	right=18mm,
	top=8mm,
}
\usepackage{amsmath}
\newcommand{\inlineeqnum}{\refstepcounter{equation}~~\mbox{(\theequation)}}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.14}
\usepackage{longtable}
\usepackage{tabu}
\usepackage{hyperref}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Imbalanced learning constitutes a recurring problem in machine learning. Frequently, practitioners and researchers have access to large amounts of data but its imbalanced nature hampers the possibility of building accurate robust predictive models. Many problems are characterized by the rare nature of the cases of interest, in domains such as health, security, quality control business applications, it is frequent to have huge populations of negative cases and just a very limited number of positive ones. The prevalence of the imbalanced learning problem explains why it continues to be a very active research topic, which in fact has grown in recent years. It is known that imbalance can exist both between-classes (the imbalance occurring between the two classes) and also within-classes (the imbalance occurring between sub-clusters of each class). In this later case traditional oversamplers will tend to perform poorly. In this paper we focus on testing the relevancy of using clustering procedures in combination with oversamplers to improve the quality of the results. We perform a series of extensive tests using the most popular oversamplers, with and without a preceding clustering procedure. The tests confirm that the use of a clustering procedure prior to the application of an oversampler will improve the results.
\end{abstract}

\section{Introduction}

Imbalanced learning can be described as the condition in which there is a significant difference between examples of different classes, in other words, when the classes are significantly skewed. Imbalanced learning is a widespread problem in the application of supervised learning in many different domains and applications (Fernandez, A., Garcia, S., Herrera, F., & Chawla, N. V. (2018). SMOTE for Learning from Imbalanced Data: Progress and Challenges, Marking the 15-year Anniversary. Journal of Artificial Intelligence Research, 61, 863–905. https://doi.org/10.1613/jair.1.11192; Haixiang, G., Yijing, L., Shang, J., Mingyun, G., Yuanyue, H., & Bing, G. (2017). Learning from class-imbalanced data: Review of methods and applications. Expert Systems with Applications, 73, 220–239. https://doi.org/10.1016/j.eswa.2016.12.035) and can be seen as a particular type of data scarcity.

Imbalanced learning affects the performance of classifiers due to two main reasons. The first, relates with the fact that, by design, standard learning methods are biased towards the majority class. This happens because, during the training phase, most learning algorithms assign similar costs to misclassification errors on the majority and minority classes. Consequently, the most abundant class tends to dominate the process, as the minority classes contribute less to the maximisation of the objective function (i.e. accuracy). The second, relates with the fact that to learn a classifier the learner needs to be able to discriminate the classes; thus, having a small number of examples from one of the classes will make the learning task much more challenging.

Another relevant issue in imbalanced learning is related with misclassification costs. While learning a classifier in training both errors (i.e. misclassification of the majority and minority classes) are treated as similar, this is seldom the case in when the classifier is  deployed for use. The norm, in most real world applications, is that the misclassification of positive cases (i.e. minority class) is much more expensive than the misclassifications of the negative cases (i.e. the majority class). This is the rule rather than the exception, in medical diagnoses or fraud detection, for instance, misclassifying positive cases as negative ones is much more expensive than the inverse situation (REF NEEDED).

We can divide the options to handle the negative effects of imbalance learning into three main approaches \cite{Fernandez2013}. The first one is by means of changing the cost function of the algorithm (G. Wu and E. Y. Chang, "KBA: kernel boundary alignment considering imbalanced data distribution," in IEEE Transactions on Knowledge and Data Engineering, vol. 17, no. 6, pp. 786-795, June 2005. doi: 10.1109/TKDE.2005.95), so that it severely penalizes false negatives. Changing the cost function can be a painstaking process, especially if the user is considering making use of several different algorithms, a common practice nowadays. A second approach relies on designing new or changing existing algorithms to take into account the relevance of the minority class (Chawla, Nitesh V., Cieslak, D. A., Hall, L. O., & Joshi, A. (2008). Automatically countering imbalance and its empirical relationship to cost. Data Mining and Knowledge Discovery, 17(2), 225–252. https://doi.org/10.1007/s10618-008-0087-0). Again, this can be a difficult and lengthy procedure, restricting the algorithm options available to the user. Finally, the last option is to modify the data, by including a preprocessing step to correct the skewness of the class distribution (REF). This can be done by generating new synthetic data instances of the minority classes, a process usually referred to as oversampling, or by removing instances from the majority class, known as undersampling. The advantage of this last option is that, by dealing with the problem at the data level, once the imbalance is corrected, the user can use any (and as many) algorithm, without need for any further changes. In this paper we focus on the oversampling strategy because it allows for the use of any algorithm without any need change it and, contrary to undersampling, it doesn’t discard any available information.

For the purposes of oversampling it is important to understand that the imbalance learning problem encompasses two different sub-problems: between-class imbalance and within-class imbalance (Jo, T., & Japkowicz, N. (2004). Class Imbalances Versus Small Disjuncts. SIGKDD Explor. Newsl., 6(1), 40–49. https://doi.org/10.1145/1007730.1007737). Between-class refers to the skewness in the distribution between majority and minority classes. Within-class imbalance is a less obvious, but equally important, problem and refers to the possible existence of several sub-clusters of minority or majority instances. Closely related with within-class imbalance is the "small disjuncts problem". Small disjuncts (Galar, M., Fernandez, A., Barrenechea, E., Bustince, H., & Herrera, F. (2012). A Review on Ensembles for the Class Imbalance Problem: Bagging-, Boosting-, and Hybrid-Based Approaches. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 42(4), 463–484. https://doi.org/10.1109/TSMCC.2011.2161285; Weiss, G. M., & Provost, F. (2003). Learning When Training Data are Costly: The Effect of Class Distribution on Tree Induction. Journal of Artificial Intelligence Research, 19, 315–354. https://doi.org/10.1613/jair.1199) that occurs when the minority concept is represented by subconcepts, usually small subclusters scattered through the input space. This adds complexity to the imbalance learning problem as it is necessary to guaranty that all the subconcepts are represented in the data such that they can be learned by the algorithm.

Many oversampling methods have been proposed and have proven to be effective in real-word domains. Particularly relevant is SMOTE (Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. (2002). SMOTE: Synthetic Minority Over-sampling Technique. Journal of Artificial Intelligence Research, 16, 321–357. https://doi.org/10.1613/jair.953), which was the first algorithm proposed and which considerably improved upon random oversampling. Since the initial SMOTE paper many variations have been proposed in order to improve some of the weaknesses of the original algorithm. However, many of these approaches suffer either from being too complex and not readily available to practitioners and researchers or being too focused on a single vulnerability of SMOTE.

In order to mitigate the within-class imbalance problem, some algorithms proposed the use of clustering techniques before the oversampling procedure. The use of clustering should enable the oversampling algorithms to identify and target areas of the input space where the generation of artificial data is most effective and needed. This way, it is expected that the algorithms are able to address, both, between-class and within-class imbalances, while at the same time avoiding the generation of noisy samples. The goal of this paper is to assess the impact and effectiveness of adding a clustering procedure, prior to the oversampling step, in the imbalanced learning problem. The results of this extra clustering step and its value are explored by analyzing how each individual oversampling algorithm, with and without the clustering step, fares against each other. 

The remainder of this work is organized as follows. In section 2, we summarize the related literature and introduce some of the currently available oversampling methods, focusing on oversamplers that employ a clustering procedure. In section 3, the experimental framework is presented, while in section 4 the experimental results are presented. Finally, in section 5 we present the main conclusions of the study.

\section{State of the art}

As noted in the introduction the approaches to deal with imbalanced learning tasks are broadly divided into three main groups: changing the cost function of the algorithm (G. Wu and E. Y. Chang, "KBA: kernel boundary alignment considering imbalanced data distribution," in IEEE Transactions on Knowledge and Data Engineering, vol. 17, no. 6, pp. 786-795, June 2005. doi: 10.1109/TKDE.2005.95); designing new or changing existing algorithms to take into account the relevance of the minority class (Chawla, Nitesh V., Cieslak, D. A., Hall, L. O., & Joshi, A. (2008); modify the data, by including a preprocessing stage to correct the skewness of the class distribution (REF). Therefore, there are two options that operate at the level of the algorithm and the other one operates ate the data level. Clearly, this last option is more general, in the sense that once the data imbalance issues are solved any “off-the shelf” algorithm can be used, without the need for any additional modification. 

The aforementioned data level solutions include different forms of resampling. Resampling is the process of manipulating the distribution of the training examples in an effort to improve the performance of classifiers (Jo & Japkowicz, 2004). Generally, resampling methods can be categorized as undersampling and oversampling. Undersampling reduces the number of majority class samples by removing samples from the training set. On the other hand, oversampling works by generating synthetic examples for the minority class and adding them to the training set. It has been shown that both oversampling and undersampling can be effective depending on the specific problem that is being addressed (Chawla et al., 2002). Both of these methods can be further categorized into random and heuristic approaches. It might be argued that while oversampling adds information to the problem, undersampling excludes information from the learning process which may negatively affect the performance of the classifier in cases where the data set is small (He, Bai, Garcia, & Li, 2008). On the other hand, depending on the quality of the generation procedure, oversampling, by generating synthetic examples, can lead to overfitting.

Several oversampling techniques have been proposed and studied in the past (MANY REFS). The simplest approach, which has been proven to perform well, is as Random Oversampling. This method works uninformed and aims to balance class distribution through the random replication of minority class examples. Given that the examples are merely replicated, the likelihood of overfitting occurring increases significantly (Batista, Prati, & Monard, 2004). 
In 2002, as an attempt to add information to the training data, Chawla et al. (Chawla et al., 2002)proposed an alternative oversampling method called SMOTE (Synthetic Minority Oversampling Technique). Instead of replicating existing observations, synthetic samples are generated. This is achieved by linear interpolation between a randomly selected sample of the minority class and one of its minority neighboring observations  (Georgios Douzas, Bacao, & Last, 2018; Fernández et al., 2018; Liu, Ghosh, & Martin, 2007). SMOTE was the first method proposed to improve random oversampling and still is the most popular oversampling method. 

However, it is important to note that the skewed class distribution (i.e. between-class imbalance) is not the only difficulty posed by imbalanced datasets to deteriorate performance in supervised learning algorithms. The distribution of the data within each class (i.e. within-class imbalance) is also relevant. 

The fact that SMOTE randomly chooses a minority instance to oversample with uniform probability allows for an effective solution combating between-class imbalance, leaving other issues such as within-class and small disjuncts unsolved. Input areas containing a large number of minority samples have a high probability of being populated further, while there can be underrepresented concepts located in small areas of the data that are likely to remain sparse (Fernández et al., 2018). Another concern is the fact that the method is susceptible to noise generation because it doesn’t distinguish overlapping class regions from so-called safe areas (REF).

Despite its weaknesses, the SMOTE is still considered the standard in the framework of learning from imbalanced data. In order to mitigate its disadvantages and improve its performance under the different possible situations, several modifications and extensions have been proposed throughout the years. They usually address a specific weakness from the original method, such as emphasizing certain minority class regions, combating within-class imbalance, or even attempting to avoid noise generation (Last et al., 2017).

The most frequent properties exploited by the techniques are the initial selection and adaptive generation of synthetic examples. Filtering is becoming more common in recent years, as well as the use of kernel functions. Regarding the interpolation procedure, it is also usual to replace the original method with other more complex ones, such as clustering-based or derived from a probabilistic function (Fernández et al., 2018). 
Safe-Level SMOTE modifies the SMOTE algorithm by applying a weight degree, the safe level, in the data generation process. The safe level provides a scale to differentiate between noisy and safe instances (Bunkhumpornpat, Sinapiromsaran, & Lursinsap, 2009). 
Similarly, there are two other enhancements of SMOTE called Borderline SMOTE1 and Borderline SMOTE2, in which only the minority examples near the borderline are oversampled. For the minority class, experiments show that our approaches achieve better TP rate and F-value than SMOTE and random over-sampling methods (Han, Wang, & Mao, 2005). Along with this variation, MWMOTE (Majority Weighted Minority Oversampling Technique for Imbalanced Data Set Learning) (Barua, Islam, Yao, & Murase, 2014; He et al., 2008), and its variation KernelADASYN (Tang & He, 2015)aim to achieve the same result. 
G-SMOTE (Georgios Douzas & Bacao, 2017a) improves the diversity of generated samples by linearly interpolating generated samples between two minority class instances. G-SMOTE extends the linear interpolation mechanism by introducing a geometric region where the data generation process occurs. 
The methods above described deal with the between-class imbalance problem and as previously mentioned, there can be two kinds of imbalance present in a data set. To solve this, there are clustering-based methods proposed to effectively reduce not only the between-class imbalance but also the within-class imbalance and to oversample the data set by rectifying these two types of imbalances simultaneously. Firstly, they divide the input space into clusters and in a posterior phase use sampling methods to adjust the size of the newly built clusters (Georgios Douzas & Bacao, 2017b; Jo & Japkowicz, 2004). 
Among these clustering-based approaches there is cluster-based oversampling. The algorithm �applies random oversampling, after clustering the training examples in the minority and the majority classes, so that the majority and minority classes are of the same size (Jo & Japkowicz, 2004). Nonetheless, since the approach does not generate new data and it merely replicates already existing samples, it is prone to overfitting.
Cluster-SMOTE  (Cieslak, Chawla, & Striegel, 2006)initially applies the k-means algorithm to the minority class and then SMOTE is used in the clusters in order to generate artificial data. Similarly, DBSMOTE (Bunkhumpornpat, Sinapiromsaran, & Lursinsap, 2012)uses the DB-SCAN algorithm to discover arbitrarily shaped clusters and then generates synthetic instances along a shortest path from each minority class instance to a pseudo-centroid of the cluster. As a more sophisticated approach, there is A-SUWO (Nekooeimehr & Lai-Yuen, 2016)which creates clusters of the minority class instances with a size, which is determined using cross-validation and generates synthetic instances based on a proposed weighting system.
SOMO (Self-Organizing Map Oversampling) (Georgios Douzas & Bacao, 2017b)creates a two- dimensional representation of the input space and based on it, applies the SMOTE procedure to generate intra-cluster and inter-cluster synthetic data, preserving the underlying manifold structure. Another clustering-based approach called CURE-SMOTE (Ma & Fan, 2017)uses the hierarchical clustering algorithm CURE to cluster the samples of the minor class and remove the noise and outliers before applying SMOTE. The goal of the method is to eliminate the noise points at the end of the process and reduce the complexity because there is no need to eliminate the farthest generated artificial samples after the SMOTE algorithm runs. While it avoids noise generation, possible imbalances within the minority class are ignored. 
Consequently, another clustering-based approach that was introduced named K-Means SMOTE (Last et al., 2017)employs the popular k-means clustering algorithm in conjunction with SMOTE oversampling in order to avoid the generation of noise by oversampling only in safe area and shifting its focus not only to fix between-class imbalance but also within-class imbalance. The method attempts to deal with the small disjuncts problem by inflating sparse minority areas and is easily implemented due to its simplicity and the widespread availability of both k-means and SMOTE.

\bibliography{references}
\bibliographystyle{apalike}

\end{document}

\documentclass[remotesensing,article,submit,moreauthors,pdftex]{Definitions/mdpi}

\firstpage{1}
\makeatletter
\setcounter{page}{\@firstpage}
\makeatother
\pubvolume{xx}
\issuenum{1}
\articlenumber{5}
\pubyear{2019}
\copyrightyear{2019}
\history{Received: date; Accepted: date; Published: date}

\usepackage{colortbl}
\usepackage{breakcites}
\usepackage{float}
\usepackage{graphicx}
\usepackage{amsmath}
\newcommand{\inlineeqnum}{\refstepcounter{equation}~~\mbox{(\theequation)}}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.14}
\usepackage{longtable}
\usepackage{tabu}
\usepackage{hyperref}

\Title{Imbalanced Learning in Land Cover Classification:  \\ \LARGE{Improving minority classes' prediction accuracy using the Geometric SMOTE algorithm}}
\Author{Georgios Douzas $^{1}$, Fernando Bacao$^{1}$*, Joao Fonseca $^{1,\dagger}$ and Manvel Khudinyan $^{1,\dagger}$}
\AuthorNames{Georgios Douzas, Fernando Bacao, Joao Fonseca and Manvel Khudinyan}
\address{
$^{1}$ \quad NOVA Information Management School; gdouzas@icloud.com, \{bacao, jpfonseca, mkhudinyan\}@novaims.unl.pt}
\corres{Correspondence: bacao@novaims.unl.pt; Tel.: +351 21 382 8610}
\firstnote{These authors contributed equally to this work.}

\abstract{
In spite of its importance in sustainable resource management, the automatic
production of Land Use/Land Cover maps continues to be a challenging problem.
The ability to build robust automatic classifiers able to produce accurate maps
can have a significant impact in the way we manage and optimize natural
resources. The difficulty in achieving these results comes from many different
factors, such as data quality, uncertainty, among others. In this paper, we
address the imbalanced learning problem, a common and difficult problem in
remote sensing that significantly affects the quality of classifiers. Having
very asymmetric distributions of the different classes constitutes a significant
hurdle for any classifier. In this work, we propose Geometric SMOTE (G-SMOTE) as
a means of addressing the imbalanced learning problem in remote sensing. G-SMOTE
is a sophisticated oversampler which significantly increases the quality of the
generated instances over previous methods, such as Synthetic Minority
Oversampling Technique (SMOTE). In this paper, we test the performance of
G-SMOTE, in the the LUCAS dataset, against other oversamplers with different
classifiers. The results show that G-SMOTE significantly outperforms all the
other oversamplers and improves the robustness of the classifiers. These results
indicate that, when using imbalanced datasets, remote sensing researchers should
consider the use of these new generation oversamplers to increase the quality of
the classification results.
}

% Keywords
\keyword{Imbalance learning; LULC classification; Oversampling;
Geometric-SMOTE; Class imbalance;}

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%\setcounter{secnumdepth}{4}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
The production of accurate Land Use/Land Cover (LULC) maps offers unique
monitoring capabilities within the remote-sensing domain \cite{Mellor2015}. LULC
maps are being used for a variety of applications, ranging from environmental
monitoring, land change detection, natural hazard assessment up to agriculture
and water/wetland monitoring \cite{Khatami2016}.

Multispectral images are an important resource to build LULC maps, allowing for
the use of classification algorithms, both supervised and unsupervised, to
automate the production of these maps. Although significant progress has been
made in the use of supervised learning techniques for automatic image
classification \cite{Tewkesbury2015}, the acquisition of labeled training sets
continues to be a bottleneck \cite{Rajan2008}. In order to build accurate and
robust supervised classifiers it is crucial to have a large enough training
dataset. Often, the problem is that different land cover types have very
different levels of area coverage, which causes some of them to be frequent in
the training dataset, while others are limited \cite{Feng2019}. In
remote-sensing classification problems, the number of instances of some classes
is often smaller when compared to the rest of the classes. This is especially
true in LULC data \cite{Williams2009, Cenggoro2018}.

The above mentioned asymmetry in class distribution affects negatively the
performance of classifiers. In the machine learning community, the problem is
referred as the imbalanced learning problem \cite{Chawla2004}. Generally, the
imbalanced learning problem refers to a skewed distribution of data across
classes in both binary and multi-class problems \cite{Abdi2016}. Particularly,
the later appears to be an even more challenging task \cite{Garcia2018}. In both
cases, during the learning phase, the minority class(es) contribute less to the
minimization of accuracy, the typical objective function, inducing a bias
towards the majority class. Consequently, as typical classification algorithms
are designed to work with reasonably balanced datasets, defining the proper
decision boundaries between the different classes becomes a very difficult task
\cite{Saez2016}.

The possible approaches to deal with the class imbalance problem can be divided into three main
groups \cite{Fernandez2013}:

\begin{enumerate}

	\item Cost-sensitive solutions. They introduce a cost matrix that applies
	higher misclassification costs for the examples of the minority class.

	\item Algorithmic level solutions. They modify the algorithmic procedure to
	reinforce the learning of the minority class.

	\item Data level solutions. They rebalance class distribution by generating
	artificial data for the minority class(es).

\end{enumerate}

The later method constitutes a more general approach since it can be used for
any classification algorithm and it does not require any type of domain
knowledge in order to construct a cost matrix.

There are several data level solutions to deal with the imbalanced learning
problem, which can be divided into three groups:

\begin{enumerate}

	\item Undersampling algorithms reduce the size of the majority classes.

	\item Oversampling algorithms attempt to even the distributions by
	generating artificial data for the minority class(es).

	\item Hybrid approaches use both oversampling and undersampling techniques
	to ensure a balanced dataset.

\end{enumerate}

Studies suggest that the usage of data level solutions in remote sensing
problems, to balance class distribution, seem to outperform models trained by
randomly resampling from the training set, an approach know as Random
Oversampling \cite{Wang2019, Mellor2015}.  Studies employing informed
oversampling methods seem to consistently yield better results
\cite{Johnson2013, Geib2015} when compared to no oversampling. Other studies
employ active learning methods such as Margin Sample by Closest Support Vector
(MS-cSV), as it benefits from avoiding oversampling in dense regions close to
the margin and samples all the feature space equivalently \cite{Tuia2009}. In
spite of the potential benefits that oversampling techniques can bring to the
improvement of automatic classification, there are not many studies on the
subject in the remote sensing domain.

In this paper, we compare the performance of various oversampling algorithms on
an the publicly available EUROSTAT's Land Use/Cover Area Statistical Survey
(LUCAS) dataset \cite{LUCAS2015} with Landsat 8 dense time-series. The
experimental procedure includes a comparison of five oversamplers using five
classifiers and three evaluation metrics. More specifically the oversampling
algorithms are G-SMOTE \cite{Douzas2019}, SMOTE \cite{Chawla2002}, Borderline
SMOTE (B-SMOTE) \cite{Han2005}, Adaptive Synthetic Sampling Technique (ADASYN)
\cite{HaiboHe2008} and Random Oversampling (ROS), while no oversampling is
included as a baseline method. Results show that G-SMOTE outperforms every other
oversampling technique, for the selected evaluation metrics.

This paper is organized in 5 sections: Section 2 analyses the resampling methods,
section 3 describes the proposed methodology, section 4 shows the results and
discussion and section 5 presents the conclusions drawn from this study.

\section{Resampling methods}

Data modification through resampling has been the most popular approach to deal
with the imbalanced learning problem, in machine learning in general and remote
sensing in particular \cite{Feng2019}. As it was mentioned above, by decoupling
the imbalance problem from the classification algorithms, resampling allows the
users to apply any standard algorithm once the resampling preprocessing step is
done. This is especially convenient for users that are not machine learning
experts and want to use several classifiers. Additionally, resampling methods
can be easily adapted to multi-class imbalanced data, which is relevant for LULC
classification. In this section we present the most relevant applications of
resampling methods for remote sensing imbalanced data classification.

\subsection{Random resampling}

Random resampling refers to non-informed strategies that remove instances from
the majority class or replicate instances from the minority class. More
specifically, the selection of the data occurs in a random fashion without
exploiting any additional information.

Some of the existing remote sensing studies implement the Random % Reference
Undersampling method (RUS), which randomly reduces the number of the majority
class training samples. However, this method has the disadvantage of information
loss as it discards samples from the majority class \cite{Feng2019}. Contrary to
it, ROS that can be considered as equivalent to Bootstrapping,
avoids information loss. However, this method simply replicates randomly
selected instances of the minority class, consequently, increasing the risk of
over-fitting \cite{Krawczyk2016}. \cite{Maxwell2018} reports that balancing data
with Random Oversampling affects the classification performance differently for
various supervised models. In their paper, land cover classification with highly
imbalanced data is carried out with six different supervised classifiers.
Implementation of Random Oversampling could slightly improve the performance of
Random Forest (RF) and Support Vector Machine (SVM) classifiers. On the other
hand, it reduced the overall classification accuracy for classifiers such as
Decision Tree (DT), Artificial Neural Network (ANN), K-Nearest Neighbors (KNN)
and Boosted DT.

\subsection{Informed resampling}

In the above section, the disadvantages of RUS and ROS have been pointed out.
Informed resampling methods aim to overcome these insufficiencies. More
specifically, they use local or global information of the class distribution to
remove or generate instances. Our focus are oversampling algorithms, since the
size of the LUCAS dataset does not favor the use of undersampling approaches.
Additionally, \cite{Feng2018} carried out a comparative analysis of
undersamplers' and oversamplers' performance for land cover classification with
Rotation Forest (RoF) ensemble classifier, showing that oversampling methods
outperform undersampling methods.

SMOTE is the most popular informed oversampling method, and it has been used to
successfully deal with the class imbalance problem in land cover classification
\cite{Cenggoro2018}. In this approach the minority class is oversampled by
randomly selecting a minority class instance and generating synthetic examples
along the line segment joining it with one of its minority class neighbors.

A number of studies report significant improvements in LULC mapping accuracy
with the use of SMOTE oversampling. For instance, the variational
semi-supervised learning (VSSL) proposed by \cite{Cenggoro2018} aims to deal
with the imbalance problem in LULC mapping. VSSL is a semi-supervised learning
framework consisting of a deep generative model. It allows learning successfully
from both labeled and unlabeled samples while using SMOTE to balance the data.
\cite{Johnson2016} used OpenStreetMap crowdsourced data and Landsat time series
for LULC classification. Similarly, the application of SMOTE improved the
classification results. Other examples of the successful application of SMOTE in
remote sensing can be found in \cite{Bogner2018}, \cite{Panda2018}.

Although recent studies demonstrate the usefulness of SMOTE for remote sensing
applications, it still has some drawbacks. SMOTE algorithm has the disadvantage
of generating noisy data \cite{Douzas2017}. In order to mitigate this problem
many variations of SMOTE have been developed. B-SMOTE \cite{Han2005} is one of
the most popular SMOTE-based oversamplers. Similarly to SMOTE, it uses
$k$-nearest neighbors selection strategy. The main difference to SMOTE is that
it modifies the data generation mechanism by generating samples closer to the
decision boundary. B-SMOTE has also been reported to perform better than SMOTE
in number of studies \cite{Nguyen2009, Ramentol2012}. ADASYN is another well
known variation of SMOTE. It is based on the idea of adaptively generating
minority class instances according to their weighted distribution: more
instances are generated for those minority class instances that are harder to
learn compared to ones that are easier to learn \cite{HaiboHe2008}.

The SMOTE procedure can be decomposed in two parts: the selection strategy for
the minority class instances and the data generation mechanism. The first part
is related to the generation of noisy instances, since SMOTE selection strategy
considers all the minority samples as equivalent. The above mentioned SMOTE
variations (B-SMOTE and ADASYN) aim to deal with this problem. On the other
hand, the second part is responsible for the diversity of the artificial
instances. There are scenarios where the linear interpolation mechanism used in
SMOTE generates nearly duplicate instances that may lead to over-fitting.
G-SMOTE algorithm is an extension of the SMOTE that aims to deal with both
problems. G-SMOTE defines a flexible geometric region around each minority class
instance for synthetic data generation. The shape of this area is controlled by
a few hyper-parameters. This significantly increases the diversity of generated
instances. Furthermore, G-SMOTE is designed to avoid noisy sample generation by
modifications of the SMOTE selection strategy. G-SMOTE has been shown to
outperform SMOTE and its above mentioned variations across 69 imbalanced
datasets for various classifiers and evaluation metrics.

\section{Methodology}

This section describes the evaluation process of G-SMOTE's performance. A
description of the oversamplers, classifiers, evaluation metrics, LUCAS dataset
characteristics as well as the experimental procedure is provided.

\subsection{LUCAS dataset}

LUCAS dataset and Landsat-8 time series from 2015 were used for the experimental
procedure. 1694 ground-truth points were included as reference data. This
dataset is grouped into 8 classes and represents the main land cover types for
the study area. The later is in the continental part of north-western Portugal,
corresponding to the area covered by Landsat image from the track 204 and row
32, with coordinates of center lat: 40.336, long: -8.468.

The dataset's class distribution and Imbalance Ratio, defined as the ratio of
number of samples for the majority class over the number of samples for any of
the minority classes, is presented in Table \ref{tab:classes_distribution}.

\begin{table}[H]
	\centering
	\begin{tabular}{llrrrr}
		\toprule
		\textbf{LUCAS Category} & \textbf{Land cover type} & \textbf{Instances}
		& \textbf{Imbalance Ratio} \\
		\hline
		A & Artificial land & 131 & 5.81 \\
		B & Cropland        & 270 & 2.81 \\
		C & Woodland        & 761 & 1.00 \\
		D & Shrubland       & 296 & 2.61 \\
		E & Grassland       & 185 & 4.11 \\
		F & Bareland        & 37  & 20.56 \\
		G & Water           & 10  & 76.10 \\
		H & Wetlands        & 4   & 190.25\\
		\hline
		\textbf{Total} & & \textbf{1694} &  \textbf{190.25} \\
		\bottomrule
	\end{tabular}
	\caption{\label{tab:classes_distribution}LUCAS nomenclature and classes distribution.}
\end{table}

The remote sensing dataset includes 8 images from Landsat 8ETM+ multi-spectral
sensor. The images are Level-1 terrain corrected products (L1TP) and are
acquired in 2015 from February to September, 1 image each month. The acquisition
mode is Descending. Only bands 2, 3, 4, 5, 6 and 7 are used from each image.
Thus, each reference point from LUCAS dataset has 48 features (Table
\ref{tab:LUCAS}). Those features have the value of digital numbers of the
pixels from each spectral bands of the image time series representing LUCAS
point location.

\pgfplotstabletypeset[
begin table=\begin{longtable},
end table=\end{longtable},
col sep=comma,
header=true,
columns={Dataset name , LUCAS},
columns/Dataset name /.style={column type=l,string type},
columns/LUCAS/.style={column type=l,string type},
every head row/.style={before row=\toprule, after row=\midrule\endhead},
every last row/.style={after row=\bottomrule
\caption{\label{tab:LUCAS}Description of the LUCAS dataset.}}
]
{../analysis/dataset_descrip.csv}

\subsection{Evaluation metrics}

Amongst the many evaluation metrics existing for classifier's performance
evaluation, there are several of them that are widely used for LULC
classification accuracy assessment. Those measures are classification overall
accuracy, user's accuracy or precision and producer's accuracy or recall. Their
calculation is based on the classification confusion
matrix \cite{Liu2007}. However, the LUCAS dataset has high inter-class
variability. Hence, the minority classes can be difficult to account for by the
classifiers that are designed to maximize the overall accuracy, as the majority
classes are receiving more weight \cite{Inglada2017}. On the other hand,
\cite{He2008} states that the use of user's and producer's accuracy for class
imbalanced data is not straightforward and producer's accuracy is not sensitive
to the data distribution at all. Considering these, class specific metrics are
needed to better describe classifier's performance. For this purpose, F-score
and G-mean metrics are used to evaluate and compare the performance of the
oversampling methods. Classification overall accuracy (OA) is provided for
discussion.

\begin{itemize}

	\renewcommand\labelitemi{--}

	\item Overall Accuracy

	Classification Overall Accuracy is the number of correctly classified
	samples divided by the sum of all the samples on the confusion matrix.

	In case when the training data is imbalanced and the accuracy metric is
	utilized, the result of the grid search for the best parameters will
	indicate that the particular parameterization is the one with highest
	classification overall accuracy, which is biased towards the majority class
	and can disregard the classification of minority classes.

	\item F-score

	F-score (or F-measure) is the harmonic mean of user's accuracy (precision)
	and	producer's accuracy (recall), where user's accuracy is the fraction of
	correctly classified instances with regard to all instances classified as
	this certain class in the confusion matrix, and producer's accuracy is the
	fraction of  correctly classified instances with regard to all instances of
	that reference class. The F-score value for multi-class imbalanced data
	classification can be calculated considering the user's accuracy and
	producer's accuracy as a unweighted average of the respective values from
	all the classes.

	\begin{equation}
	F{-}score=2\frac{\text{E(UA)} \times \text{E(PA)}}{\text{E(UA)} +
	\text{E(PA)}}
	\end{equation}

	Where E(UA) and E(PA) are unweighted mean values of user's and producer's
	accuracy of all the classes, respectively.

	\item G-measure

	The geometric mean score (G-mean) is the root of the product of class-wise
	sensitivity and specificity. For the multi-class case sensitivity and
	specificity are calculated for each class, then averaged with simple mean
	value. Here the sensitivity (true positive rate) is the same with producer's
	accuracy (recall), and specificity (true negative rate) is defined as the
	proportion of actual negatives classified as such. Adequately, specificity
	can be defined as producer's accuracy of the negative class
	\cite{Silva2017}.

	\begin{equation}
	G{-}mean = \sqrt{E(Sensitivity) \times E(Specificity)}
	\end{equation}

	Where E(Sensitivity) and E(Specificity) are the unweighted mean values of
	sensitivity and specificity of all the classes, respectively. The geometric
	mean indicates the balance between classification performances on both
	majority and minority classes. It means, that high misclassification in the
	minority class will lead to a lower geometric mean value. This is a very
	valuable feature for evaluation of the classifier's performance with class
	imbalance problem, which is the case of this study.

\end{itemize}

\subsection{Machine learning algorithms}

Four oversampling techniques were used in this experiment as baseline methods
along with G-SMOTE. Random oversampling was chosen for its simplicity. SMOTE was
selected for being a classic technique and the most popular oversampling
algorithm \cite{Douzas2019}. ADASYN \cite{HaiboHe2008} and Borderline SMOTE
\cite{Han2005} were selected for representing popular modifications of the
original SMOTE technique in the selection phase and data generation mechanism,
respectively. Finally, we use no oversampling as an additional baseline method.

The selection of classification algorithms was done according to 3 criteria:
learning type, training time and popularity within the remote sensing community.
We further divide classification algorithms into 4 different learning types:
neighbors-based, rule-based, ensemble and generalized linear methods.

For each combination of oversampler and classifier, a grid search will be run in
order to find the optimal parameter settings. Adding new classifiers,
oversamplers or grid search values may exponentially increase the time necessary
to run the experiment. Therefore, in order to setup an extensive grid search and
run the full experiment within feasible time, selecting computationally
efficient classifiers becomes vital.

\cite{Khatami2016} developed a meta-analysis of classification processes out of
266 manuscripts published from 1998 to 2012. Additionally, \cite{Maxwell2018}
also describe mature classification methods used in remote sensing. Both
manuscripts were used to determine the most commonly used classifiers for LULC
classification problems: K-Nearest Neighbors, Support Vector Machines, Decision
Trees, Boosted Decision Trees, Random Forests and Neural Networks. None of the
manuscripts mention the use of generalized linear models.

Amongst the neighbors-based learning methods the KNN algorithm was selected. Out
of the rule-based learning methods, the decision tree classifier (DT) was
chosen. Two ensemble learning methods were chosen: Gradient boosting classifier
and Random Forest classifier. Finally, the logistic regression classifier was
selected amongst the generalized linear models, since it is a commonly used
baseline algorithm. All these algorithms were found to be computationally
efficient and commonly used classifiers for the proposed task (with exception
for the logistic regression).

Despite their common use, both artificial neural networks and Support Vector
Machines were not included in this experiment due to their long training times.

\subsection{Experiment settings}

As it is stated in the section 3.2, machine-learning classifiers usually have
parameters that need to be set up by users for a better classification
performance. Parameter tuning to define the optimal values of the
hyper-parameters was performed with 3-fold cross-validation (CV) for each of the
model (combining classifiers with oversampler). The best parameters are later
selected to validate the classifier's performance.

In order to avoid over-optimistic classification results, implementation of CV
with oversampling technique should be carried out as following \cite{Lusa2015}.
In $k$-fold CV dataset is divided into $k$ parts, one part is excluded and used
as only test set, while the rest $k$-1 part is used to train the model. This
process is iterated and each of the $k$ folds are used only once as a test set
to evaluate the classifier's performance. Later, results are averaged across all
$k$ iterations. It is crucial to remember that all the steps for building the
prediction model should be applied only on the training data. From this respect,
the oversampling (or any kind of data resampling) technique should not be
applied on the whole dataset. Instead, it is used only on the training dataset
generated in each step of the CV procedure.

A range of hyper-parameters used for the grid search for each of the classifier
and oversampler are presented in the table \ref{tab:grid}.

\begin{table}[H]
	\centering
	\begin{tabular}{lll}
		\toprule
		Classifier       & Hyper-Parameters & Values\\
		\hline
		LR               & maximum iterations   & 10000   \\
		KNN              & number of neighbors  & {3, 5} \\
		DT               & maximum depth        & {3, 6} \\
		GBC              & maximum depth        & {3, 6} \\
    			 		 & number of estimators & {50, 100} \\
		RF               & maximum depth        & {None, 3, 6} \\
				 		 & number of estimators & {50, 100} \\
		\toprule
		Oversampler      &                      & \\
		\hline
		G-SMOTE          & number of neighbors  & {3, 5} \\
				 		 & selection strategy   & combined, minority, majority\\
				 		 & truncation factor    & {-1.0, -0.5, .0, 0.25, 0.5,
				 		 0.75, 1.0} \\
				 		 & deformation factor   & {0, 0.2, 0.4, 0.5, 0.6, 0.8,
				 		 1.0} \\
 		SMOTE            & number of neighbors & {3, 5} \\
		BORDERLINE SMOTE & number of neighbors & {3, 5} \\
		ADASYN           & number of neighbors & {2, 3} \\
		\bottomrule
	\end{tabular}
	\caption{\label{tab:grid}User-defined grid values}
\end{table}

\subsection{Software implementation}

The implementation of the experimental procedure was based on the Python
programming language, using the Scikit-Learn \cite{Pedregosa2011}, Imbalanced-
Learn \cite{JMLR:v18:16-365} and Geometric-SMOTE \cite{Douzas2019} libraries.
The experiments reported in this paper as well as the analysis of the results
are reproducible using the scripts available at \url{https://github.com/AlgoWit/
publications}.

\section{Results and discussion}

This section presents the final results of classifiers performance with
different oversamplers over LUCAS dataset. A simple ranking of oversamplers
performance is carried out for comparative analysis.

The cross-validation scores for each combination of classifiers, evaluation
metrics and oversamplers are presented in Table \ref{tab:mean_cross_val}. From
this table we can observe that Geometric-SMOTE over-performs nearly all other
oversampling methods for both G-mean and F-score metrics on all classifiers. The
absolute best results are achieved when Geometric-SMOTE is combined with Fandom
Forest.

\pgfplotstabletypeset[
begin table=\begin{longtable},
end table=\end{longtable},
col sep=comma,
header=true,
columns={Classifier,Metric,NO OS,RAND OS,SMOTE,B-SMOTE,ADASYN,G-SMOTE},
columns/Classifier/.style={column type=l,string type},
columns/Metric/.style={column type=l,string type},
columns/NO OS/.style={column type=l,string type},
columns/RAND OS/.style={column type=l,string type},
columns/SMOTE/.style={column type=l,string type},
columns/B-SMOTE/.style={column type=l,string type},
columns/ADASYN/.style={column type=l,string type},
columns/G-SMOTE/.style={column type=l,string type},
every head row/.style={before row=\toprule, after row=\midrule\endhead},
every last row/.style={after row=\bottomrule
\caption{\label{tab:mean_cross_val}Results for cross validation scores of
oversamplers (NO OS: No Oversampling, RAND OS: Random Oversampling,
B-SMOTE: Borderline SMOTE, G-SMOTE: Geometric SMOTE)}}
]
{../analysis/mean_scores.csv}

Amongst the classifiers Random Forest shows best performance for all three
measures. It recorded constantly improved results for both G-mean and F-score
metrics while combined with oversamplers. Second best classification results are
obtained with Gradient Boosting Classifier, and the worst performance is with
single Decision Tree Classifier.

Somewhat contrary to expectations, for some of the data balancing methods
slightly decreased the accuracy of imbalance-specific metrics while compared to
the baseline methods. For instance, the KNN classifier recorded higher G- mean
value (0.50) with unchanged data compared to the results of Random Oversampling
(0.48), ADASYN (0.48) and SMOTE (0.49). Similarly, Logistic Regression
classifier recorded higher F-score value (0.30) with No Oversampling compared to
the ones with Random Oversampling (0.29), SMOTE (0.29) and ADASYN (0.28).

A ranking score was assigned to each oversampling method with the best and worst
performing methods receiving scores equal to 1 and 6, respectively. This
comparison is presented in Table \ref{tab:ranking}. From this table we can
observe that G-SMOTE systematically outperforms any other oversampling algorithm
according to the G-mean measure. Using F-score it performs best on all
classifiers except the Decision Tree classifier, where Borderline-SMOTE
outperforms the remaining oversamplers. Regardless, its discrepancy with
Geometric-SMOTE is negligible.

\pgfplotstabletypeset[
begin table=\begin{longtable},
end table=\end{longtable},
col sep=comma,
header=true,
columns={Classifier,Metric,NO OS,RAND OS,SMOTE,B-SMOTE,ADASYN,G-SMOTE},
columns/Classifier/.style={column type=l,string type},
columns/Metric/.style={column type=l,string type},
columns/NO OS/.style={column type=l,string type},
columns/RAND OS/.style={column type=l,string type},
columns/SMOTE/.style={column type=l,string type},
columns/B-SMOTE/.style={column type=l,string type},
columns/ADASYN/.style={column type=l,string type},
columns/G-SMOTE/.style={column type=l,string type},
every head row/.style={before row=\toprule, after row=\midrule\endhead},
every last row/.style={after row=\bottomrule
\caption{\label{tab:ranking}Results for ranking of oversamplers}}
]
{../analysis/mean_ranking.csv}

The mean of ranking scores for each of oversampling method across the
classification algorithms is presented in the Table \ref{tab:mean_rankings}.
From this table it is easier to interpret that Geometric-SMOTE has been the best
method almost all the time for G-mean and F-score measures. The same way
Borderline-SMOTE has been the second best method almost all the times. In
contrast to those two methods, in average ADASYN could not improve the
classification results for any of measures compared to SMOTE method.

Accuracy scores' results depict the bias discussed in section \ref{evaluation
methods}. This metric, by attributing proportional importance to the predictive
power of each class according to the dataset's class distribution, hinders the
importance of minority classes. Therefore, this metric can be misleading for
imbalanced data classification problems. Accordingly, models with no
oversampling present the highest accuracy score. In a multi-class classification
problem with an imbalanced dataset, where the prediction of all the classes are
of equal importance, overall accuracy should not be implemented to evaluate the
model performance, as is commonly utilized in remote sensing. However, even for
accuracy metric, Geometric-SMOTE shows the best performance amongst the data
modification algorithms and exceeds the rest of the oversampling methods.

\pgfplotstabletypeset[
begin table=\begin{longtable},
end table=\end{longtable},
col sep=comma,
header=true,
columns={Oversampler,F1 macro,G-mean macro,Accuracy},
columns/Oversampler/.style={column type=l,string type},
columns/F1 macro/.style={column type=l,string type},
columns/G-mean macro/.style={column type=l,string type},
columns/Accuracy/.style={column type=l,string type},
every head row/.style={before row=\toprule, after row=\midrule\endhead},
every last row/.style={after row=\bottomrule
\caption{\label{tab:mean_rankings}Results for mean ranking of oversamplers}}
]
{../analysis/model_mean_ranking.csv}

Table \ref{tab:smote_gsmote} presents the percentage difference between
Geometric-SMOTE and SMOTE. For this comparison SMOTE algorithm was selected due
to its popularity and wide use within the remote sensing society (see section
2). The table shows that Geometric-SMOTE outperforms SMOTE regardless of the
classifier or metric used.

\pgfplotstabletypeset[
begin table=\begin{longtable},
end table=\end{longtable},
col sep=comma,
header=true,
columns={Classifier,Metric,Difference},
columns/Classifier/.style={column type=l,string type},
columns/Metric/.style={column type=l,string type},
columns/Difference/.style={column type=l,string type},
every head row/.style={before row=\toprule, after row=\midrule\endhead},
every last row/.style={after row=\bottomrule
\caption{\label{tab:smote_gsmote}Results for percentage difference
between G-SMOTE and SMOTE}}
]
{../analysis/mean_perc_diff_scores.csv}

\section{Conclusions}

In this paper we presented the performance of Geometric-SMOTE oversampling
algorithm as a novel approach to deal with class imbalance problem in naturally
imbalanced remote sensing datasets  with an aim to improve the LULC
classification. Geometric-SMOTE's performance was evaluated and compared with
other oversampling methods, specifically Random Oversampling, SMOTE and its
variations Borderline-SMOTE and ADASYN, through the use of a number of
classifiers: Logistic Regression, K-Nearest Neighbors, Decision Trees, Gradient
Boosting Classifier and Random Forests.

For this purpose, an imbalanced LUCAS dataset was used. For model validation and
comparison Overall Accuracy was used, as well as G-mean and F-score as they
represent imbalance-specific metrics. The models were trained using the 3-fold
cross-validation method. The experiment results show the importance of
oversampling methods for multi-class imbalanced data classification as in most
of the cases balancing data with oversampling resulted for higher G-mean and F-
score values.

Geometric-SMOTE shows the best results and dominated the ranking across the
oversamplers. However, table \ref{tab:ranking} reports Decision Trees to achieve
a higher F-Score when using the Borderline SMOTE algorithm. Although this
algorithm systematically performs better than any other oversampling method
using G-mean and F-score metrics, it may be useful to also consider Borderline
SMOTE. Geometric-SMOTE proves to be a useful tool for remote sensing researchers
and can effectively replace the two widely accepted oversamplers Random
Oversampling and SMOTE methods.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{Conceptualization, F.B.; Methodology, G.D.; Software,
G.D.; Validation, F.B., G.D.; Formal Analysis, J.F and M.K.; Writing -
Original Draft Preparation, M.K., J.F.; Writing - Review \& Editing, F.B., G.D.;
Supervision, F.B.; Funding Acquisition, F.B.}

\funding{This research was funded by NAME OF FUNDER grant number XXX.}

\acknowledgments{The authors would like to thank Direção Geral do Território
(DGT) for supporting the used data in this study.}

\conflictsofinterest{The authors declare no conflict of interest. The funders
had no role in the design of the study; in the collection, analyses, or
interpretation of data; in the writing of the manuscript, or in the decision to
publish the results.}

\abbreviations{The following abbreviations are used in this manuscript:\\
\noindent
\begin{tabular}{@{}ll}
  OS & Oversampling\\
  CV & Cross-Validation\\
  LULC & Land Use/Land Cover\\
  LUCAS & Land Use/Cover Area Statistical Survey\\
  SMOTE & Synthetic Minority Over-sampling Technique\\
  ADASYN & Adaptive Synthetic Sampling Technique\\
  LR & Logistic Regression\\
  KNN & K-Nearest Neighbors\\
  DT & Decision Trees\\
  GBC & Gradient Boosting Classifier\\
  RF & Random Forest\\
\end{tabular}}


\reftitle{References}

\externalbibliography{yes}
\bibliography{references}

\end{document}

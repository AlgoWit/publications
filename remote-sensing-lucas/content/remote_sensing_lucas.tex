\documentclass[remotesensing,article,submit,moreauthors,pdftex]{Definitions/mdpi}

\firstpage{1}
\makeatletter
\setcounter{page}{\@firstpage}
\makeatother
\pubvolume{xx}
\issuenum{1}
\articlenumber{5}
\pubyear{2019}
\copyrightyear{2019}
\history{Received: date; Accepted: date; Published: date}

\usepackage{colortbl}
\usepackage{breakcites}
\usepackage{float}
\usepackage{graphicx}
\usepackage{amsmath}
\newcommand{\inlineeqnum}{\refstepcounter{equation}~~\mbox{(\theequation)}}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.14}
\usepackage{longtable}
\usepackage{tabu}
\usepackage{hyperref}

\Title{Imbalanced Learning in Land Cover Classification:  \\ \LARGE{Improving
minority classes' prediction accuracy using the Geometric SMOTE algorithm}}
\Author{Georgios Douzas $^{1}$, Fernando Bacao$^{1}$*, Joao Fonseca
$^{1,\dagger}$ and Manvel Khudinyan $^{1,\dagger}$} \AuthorNames{Georgios
Douzas, Fernando Bacao, Joao Fonseca and Manvel Khudinyan} \address{$^{1}$ \quad
NOVA Information Management School; gdouzas@icloud.com, \{bacao, jpfonseca,
mkhudinyan\}@novaims.unl.pt}
\corres{Correspondence: bacao@novaims.unl.pt; Tel.: +351 21 382 8610}
\firstnote{These authors contributed equally to this work.}

\abstract{In spite of its importance in sustainable resource management, the
automatic production of Land Use/Land Cover maps continues to be a challenging
problem. The ability to build robust automatic classifiers able to produce
accurate maps can have a significant impact in the way we manage and optimize
natural resources. The difficulty in achieving these results comes from many
different factors, such as data quality, uncertainty, among others. In this
paper, we address the imbalanced learning problem, a common and difficult
problem in remote sensing that significantly affects the quality of classifiers.
Having very asymmetric distributions of the different classes constitutes a
significant hurdle for any classifier. In this work, we propose Geometric SMOTE
(G-SMOTE) as a means of addressing the imbalanced learning problem in remote
sensing. G-SMOTE is a sophisticated oversampler which significantly increases
the quality of the generated instances over previous methods, such as Synthetic
Minority Oversampling Technique (SMOTE). In this paper, we test the performance
of G-SMOTE, in the the LUCAS dataset, against other oversamplers with a variety
of classifiers. The results show that G-SMOTE significantly outperforms all the
other oversamplers and improves the robustness of the classifiers. These results
indicate that, when using imbalanced datasets, remote sensing researchers should
consider the use of these new generation oversamplers to increase the quality of
the classification results.}

\keyword{Imbalance learning; LULC classification; Oversampling;
Geometric-SMOTE; Class imbalance;}

\begin{document}

\section{Introduction}
The production of accurate Land Use/Land Cover (LULC) maps offers unique
monitoring capabilities within the remote-sensing domain \cite{Mellor2015}. LULC
maps are being used for a variety of applications, ranging from environmental
monitoring, land change detection, natural hazard assessment up to agriculture
and water/wetland monitoring \cite{Khatami2016}.

Multispectral images are an important resource to build LULC maps, allowing for
the use of classification algorithms, both supervised and unsupervised, to
automate the production of these maps. Although significant progress has been
made in the use of supervised learning techniques for automatic image
classification \cite{Tewkesbury2015}, the acquisition of labeled training sets
continues to be a bottleneck \cite{Rajan2008}. In order to build accurate and
robust supervised classifiers it is crucial to have a large enough training
dataset. Often, the problem is that different land cover types have very
different levels of area coverage, which causes some of them to be frequent in
the training dataset, while others are limited \cite{Feng2019}. In
remote-sensing classification problems, the number of instances of some classes
is often smaller when compared to the rest of the classes. This is especially
true in  in LUCAS data, where a squared grid sampling strategy is used.
\cite{Williams2009, Cenggoro2018}.

The above mentioned asymmetry in class distribution affects negatively the
performance of classifiers. In the machine learning community, the problem is
referred as the imbalanced learning problem \cite{Chawla2004}. Generally, the
imbalanced learning problem refers to a skewed distribution of data across
classes in both binary and multi-class problems \cite{Abdi2016}. Particularly,
the later appears to be an even more challenging task \cite{Garcia2018}. In both
cases, during the learning phase, the minority class(es) contribute less to the
minimization of accuracy, the typical objective function, inducing a bias
towards the majority class. Consequently, as typical classification algorithms
are designed to work with reasonably balanced datasets, defining the proper
decision boundaries between the different classes becomes a very difficult task
\cite{Saez2016}.

The possible approaches to deal with the class imbalance problem can be divided into three main
groups \cite{Fernandez2013}:

\begin{enumerate}

	\item Cost-sensitive solutions. They introduce a cost matrix that applies
	higher misclassification costs for the examples of the minority class.

	\item Algorithmic level solutions. They modify the algorithmic procedure to
	reinforce the learning of the minority class.

	\item Data level solutions. They rebalance the class distribution either by
	removing instances from the majority class(es) or by generating artificial
	data for the minority class(es).

\end{enumerate}

The later method constitutes a more general approach since it can be used for
any classification algorithm and it does not require any type of domain
knowledge in order to construct a cost matrix.

There are several data level solutions to deal with the imbalanced learning
problem, which can be divided into three groups:

\begin{enumerate}

	\item Undersampling algorithms reduce the size of the majority classes.

	\item Oversampling algorithms attempt to even the distributions by
	generating artificial data for the minority class(es).

	\item Hybrid approaches use both oversampling and undersampling techniques
	to ensure a balanced dataset.

\end{enumerate}

Studies suggest that the usage of data level solutions in remote sensing
problems, to balance class distribution, seem to outperform models trained by
randomly resampling from the training set, an approach know as Random
Oversampling \cite{Wang2019, Mellor2015}.  Studies employing informed
oversampling methods seem to consistently yield better results
\cite{Johnson2013, Geib2015} when compared to no oversampling. Other studies
employ active learning methods such as Margin Sample by Closest Support Vector
(MS-cSV), as it benefits from avoiding oversampling in dense regions close to
the margin and samples all the feature space equivalently \cite{Tuia2009}. In
spite of the potential benefits that oversampling techniques can bring to the
improvement of automatic classification, there are not many studies on the
subject in the remote sensing domain.

In this paper, we compare the performance of various oversampling algorithms on
an the publicly available EUROSTAT's Land Use/Cover Area Statistical Survey
(LUCAS) dataset \cite{LUCAS2015} with Landsat 8 dense time-series. The
experimental procedure includes a comparison of five oversamplers using five
classifiers and three evaluation metrics. More specifically the oversampling
algorithms are G-SMOTE \cite{Douzas2019}, SMOTE \cite{Chawla2002}, Borderline
SMOTE (B-SMOTE) \cite{Han2005}, Adaptive Synthetic Sampling Technique (ADASYN)
\cite{HaiboHe2008} and Random Oversampling (ROS), while no oversampling is
included as a baseline method. Results show that G-SMOTE outperforms every other
oversampling technique, for the selected evaluation metrics.

This paper is organized in 5 sections: Section 2 analyses the resampling methods,
section 3 describes the proposed methodology, section 4 shows the results and
discussion and section 5 presents the conclusions drawn from this study.

\section{Resampling methods}

Data modification through resampling has been the most popular approach to deal
with the imbalanced learning problem, in machine learning in general and remote
sensing in particular \cite{Feng2019}. As it was mentioned above, by decoupling
the imbalance problem from the classification algorithms, resampling allows the
users to apply any standard algorithm once the resampling preprocessing step is
done. This is especially convenient for users that are not machine learning
experts and want to use several classifiers. Additionally, resampling methods
can be easily adapted to multi-class imbalanced data, which is relevant for LULC
classification. In this section we present the most relevant applications of
resampling methods for remote sensing imbalanced data classification.

\subsection{Random resampling}

Random resampling refers to non-informed strategies that remove instances from
the majority class or replicate instances from the minority class. More
specifically, the selection of the data occurs in a random fashion without
exploiting any additional information.

Some of the existing remote sensing studies implement the Random % Reference
Undersampling method (RUS), which randomly reduces the number of the majority
class training samples. However, this method has the disadvantage of information
loss as it discards samples from the majority class \cite{Feng2019}. Contrary to
it, ROS that can be considered as equivalent to Bootstrapping,
avoids information loss. However, this method simply replicates randomly
selected instances of the minority class, consequently, increasing the risk of
over-fitting \cite{Krawczyk2016}. \cite{Maxwell2018} reports that balancing data
with Random Oversampling affects the classification performance differently for
various supervised models. In their paper, land cover classification with highly
imbalanced data is carried out with six different supervised classifiers.
Implementation of Random Oversampling could slightly improve the performance of
Random Forest (RF) and Support Vector Machine (SVM) classifiers. On the other
hand, it reduced the overall classification accuracy for classifiers such as
Decision Tree (DT), Artificial Neural Network (ANN), K-Nearest Neighbors (KNN)
and Boosted DT.

\subsection{Informed resampling}

In the above section, the disadvantages of RUS and ROS have been pointed out.
Informed resampling methods aim to overcome these insufficiencies. More
specifically, they use local or global information of the class distribution to
remove or generate instances. Our focus are oversampling algorithms, since the
size of the LUCAS dataset does not favor the use of undersampling approaches.
Additionally, \cite{Feng2018} carried out a comparative analysis of
undersamplers' and oversamplers' performance for land cover classification with
Rotation Forest (RoF) ensemble classifier, showing that oversampling methods
outperform undersampling methods.

SMOTE is the most popular informed oversampling method, and it has been used to
successfully deal with the class imbalance problem in land cover classification
\cite{Cenggoro2018}. In this approach the minority class is oversampled by
randomly selecting a minority class instance and generating synthetic examples
along the line segment joining it with one of its minority class neighbors.

A number of studies report significant improvements in LULC mapping accuracy
with the use of SMOTE oversampling. For instance, the variational
semi-supervised learning (VSSL) proposed by \cite{Cenggoro2018} aims to deal
with the imbalance problem in LULC mapping. VSSL is a semi-supervised learning
framework consisting of a deep generative model. It allows learning successfully
from both labeled and unlabeled samples while using SMOTE to balance the data.
\cite{Johnson2016} used OpenStreetMap crowdsourced data and Landsat time series
for LULC classification. Similarly, the application of SMOTE improved the
classification results. Other examples of the successful application of SMOTE in
remote sensing can be found in \cite{Bogner2018}, \cite{Panda2018}.

Although recent studies demonstrate the usefulness of SMOTE for remote sensing
applications, it still has some drawbacks. SMOTE algorithm has the disadvantage
of generating noisy data \cite{Douzas2017}. In order to mitigate this problem
many variations of SMOTE have been developed. B-SMOTE \cite{Han2005} is one of
the most popular SMOTE-based oversamplers. Similarly to SMOTE, it uses
$k$-nearest neighbors selection strategy. The main difference to SMOTE is that
it modifies the data generation mechanism by generating samples closer to the
decision boundary. B-SMOTE has also been reported to perform better than SMOTE
in number of studies \cite{Nguyen2009, Ramentol2012}. ADASYN is another well
known variation of SMOTE. It is based on the idea of adaptively generating
minority class instances according to their weighted distribution: more
instances are generated for those minority class instances that are harder to
learn compared to ones that are easier to learn \cite{HaiboHe2008}.

The SMOTE procedure can be decomposed in two parts: the selection strategy for
the minority class instances and the data generation mechanism. The first part
is related to the generation of noisy instances, since SMOTE selection strategy
considers all the minority samples as equivalent. The above mentioned SMOTE
variations (B-SMOTE and ADASYN) aim to deal with this problem. On the other
hand, the second part is responsible for the diversity of the artificial
instances. There are scenarios where the linear interpolation mechanism used in
SMOTE generates nearly duplicate instances that may lead to over-fitting.
G-SMOTE algorithm is an extension of the SMOTE that aims to deal with both
problems. G-SMOTE defines a flexible geometric region around each minority class
instance for synthetic data generation. The shape of this area is controlled by
a few hyper-parameters. This significantly increases the diversity of generated
instances. Furthermore, G-SMOTE is designed to avoid noisy sample generation by
modifications of the SMOTE selection strategy. G-SMOTE has been shown to
outperform SMOTE and its above mentioned variations across 69 imbalanced
datasets for various classifiers and evaluation metrics.

\section{Methodology}

This section describes the evaluation process of G-SMOTE's performance. A
description of the oversamplers, classifiers, evaluation metrics, LUCAS dataset
characteristics as well as the experimental procedure is provided.

\subsection{LUCAS dataset}

LUCAS dataset and Landsat-8 time series from 2015 were used for the experimental
procedure. 1694 ground-truth points were included as reference data. This
dataset contains 8 classes that represent the main land cover types for the
study area. The later is in the continental part of north-western Portugal,
corresponding to the area covered by Landsat image from the track 204 and row
32.

The number of samples per class and the Imbalance Ratio (IR), defined as the ratio of
number of samples for the majority class over the number of samples for any of
the minority classes, is presented in Table \ref{tab:classes_distribution}.

\begin{table}[H]
	\centering
	\begin{tabular}{llrrrr}
		\toprule
		\textbf{LUCAS Category} & \textbf{Land cover type} & \textbf{Instances}
		& \textbf{IR} \\
		\hline
		A & Artificial land & 131 & 5.81 \\
		B & Cropland        & 270 & 2.81 \\
		C & Woodland        & 761 & 1.00 \\
		D & Shrubland       & 296 & 2.61 \\
		E & Grassland       & 185 & 4.11 \\
		F & Bareland        & 37  & 20.56 \\
		G & Water           & 10  & 76.10 \\
		H & Wetlands        & 4   & 190.25\\
		\hline
		\textbf{Total} & & \textbf{1694} &  \textbf{190.25} \\
		\bottomrule
	\end{tabular}
	\caption{\label{tab:classes_distribution}LUCAS nomenclature and classes distribution.}
\end{table}

The experiment's dataset includes 8 images from Landsat 8ETM+ multi-spectral
sensor. The images are Level-1 terrain corrected products (L1TP) and are
acquired in 2015 from February to September, 1 image each month. The acquisition
mode is Descending. Only bands 2, 3, 4, 5, 6 and 7 are used from each image.
Thus, each reference point from LUCAS dataset has 48 features. The features'
values are digital numbers of the pixels from each spectral band of the image
time series representing LUCAS point location. Table \ref{tab:LUCAS} presents a
description of the LUCAS dataset, including information about the majority class
C and the smallest minority class H to emphasize the imbalance character of the
dataset:

\pgfplotstabletypeset[
begin table=\begin{longtable},
end table=\end{longtable},
col sep=comma,
header=true,
columns={Dataset, LUCAS},
columns/Dataset/.style={column type=l,string type},
columns/LUCAS/.style={column type=l,string type},
every head row/.style={before row=\toprule, after row=\midrule\endhead},
every last row/.style={after row=\bottomrule
\caption{\label{tab:LUCAS}Description of the LUCAS dataset.}}
]
{../analysis/dataset_descrip.csv}

\subsection{Evaluation metrics}  \label{Evaluation metrics}

Amongst the many evaluation metrics existing for classifier's performance
evaluation, \textit{Overall Accuracy}, user's accuracy (or \textit{Precision})
and producer's accuracy (or \textit{Recall}) are the most common in LULC
classification. For a binary classification task their calculation is given in
terms of the true positives \( TP \), true negatives \(TN \), false positives
\( FP \) and false negatives \( FN \)
\cite{Liu2007}. More specifically,  \( \textit{Precision} =  \frac{TP}{TP + FP}
\) and \(\textit{Recall} =  \frac{TP}{TP + FN} \). For the multi-class case, the
average value across classes is used, as explained below.

The LUCAS dataset is highly imbalanced, having wide a range of IRs for the
different minority classes. Therefore, the use of the metrics above for class
imbalanced data is not an appropriate choice since they are mainly determined by
the majority class contribution \cite{He2009}. An appropriate evaluation metric
should consider the classification accuracy of all classes. A simple approach
for the multi-class case is to select a binary class evaluation metric, apply it
to each binary sub-task of the multi-class problem i.e. consider each class
versus the rest and finally average its values. For this purpose,
\textit{F-score} and \textit{G-mean} metrics are used to evaluate and compare
the performance of the oversampling methods while \textit{Overall Accuracy} is
provided for discussion:

\begin{itemize}

	\renewcommand\labelitemi{--}

	\item The \textit{Overall Accuracy} is the number of correctly classified
	samples for classes divided by the sum of all samples. Assuming that the
	various classes are labeled by the index \( c \), \textit{Overall Accuracy}
	is given by the following formula:

	$$\textit{Overall Accuracy} = \frac{ \sum\limits_{c}{ \text{TP}_{c} } }{
	\sum\limits_{c}{ (\text{TP}_{c}  + \text{FP}_{c}) } } $$

	\item The \textit{F-score} is the harmonic mean of \textit{Precision} and
	\textit{Recall}. The \textit{F-score} for the multi-class case can be
	calculated using their average per class values:

	$$\textit{F-score}=2\frac{\overline{Precision} \times \overline{Recall}}{\overline{Precision} +
	\overline{Recall}}$$

	\item The \textit{G-mean} is the geometric mean of \textit{Sensitivity} and
	\textit{Specificity}. \textit{Sensitivity} is identical to the
	\textit{Recall} while \textit{Specificity} is given by the formula
	\(\textit{Specificity} =  \frac{TN}{TN + FP} \). Therefore, they are equal
	to the true positive and true negative rate, respectively. The
	\textit{G-mean} for the multi-class case can be calculated using their
	average per class values:

	$$\textit{G-mean} = \sqrt{ \overline{Sensitivity} \times
	\overline{Specificity}}$$

\end{itemize}

\subsection{Machine learning algorithms}

The main objective of the paper is to show the effectiveness of G-SMOTE when it
is used on multi-class highly imbalanced data of a remote sensing application as
well as to compare its performance to other oversampling methods. Four oversampling
algorithms were used in the experiment along with G-SMOTE. ROS was chosen for its
simplicity. SMOTE was selected for being the most widely used oversampler
\cite{Douzas2019}. ADASYN \cite{HaiboHe2008} and Borderline SMOTE \cite{Han2005}
were selected for representing popular modifications of the original SMOTE
algorithm. Finally, no oversampling was also applied as an additional baseline
method.

For the evaluation of the oversampling methods, the classifiers LR, KNN, DT, GBC
and RFC were selected. The choice of classifiers was done according to the
following criteria: learning type, training time and popularity within the
remote sensing community. All these algorithms were found to be computationally
efficient and commonly used for the proposed task, with the exception of LR
which is rarely used in remote sensing applications \cite{Khatami2016},
\cite{Maxwell2018}.

\subsection{Experiment settings}

In order to evaluate the performance of each oversampler, every possible
combination of oversampler, classifier and metric is formed. The evaluation
score for each of the above combinations is generated through a \( n \)-fold
cross validation procedure with \( n = 5 \). Before starting the training of
each classifier, and in each stage \(i \in \{1, 2 ,... , n \} \) of the \( n
\)-fold cross validation procedure, synthetic data \( S_{i} \) were generated
using the oversampler, based on the training data \(T_{i} \) of the \( n - 1 \)
folds, such that the resulting \(S_{i} \cup T_{i} \) training set becomes
perfectly balanced. This enhanced training set in turn was used to train the
classifier. The performance evaluation of the classifiers was done on the
validation data \( V_{i} \) of the remaining fold, where \(V_{i} \cup T_{i} = D
\), \(V_{i} \cap T_{i} = \emptyset \) while \( D \) represents the dataset. The
process above is repeated three times and the results are averaged.

The range of hyper-parameters used for each classifier and oversampler are
presented in table \ref{tab:grid}:

\begin{table}[H]
	\centering
	\begin{tabular}{lll}
		\toprule
		Classifier       & Hyper-Parameters & Values\\
		\hline
		LR               & maximum iterations   & 10000   \\
		KNN              & number of neighbors  & {3, 5} \\
		DT               & maximum depth        & {3, 6} \\
		GBC              & maximum depth        & {3, 6} \\
    			 		 & number of estimators & {50, 100} \\
		RF               & maximum depth        & {None, 3, 6} \\
				 		 & number of estimators & {50, 100} \\
		\toprule
		Oversampler      &                      & \\
		\hline
		G-SMOTE          & number of neighbors  & {3, 5} \\
				 		 & selection strategy   & combined, minority, majority\\
				 		 & truncation factor    & {-1.0, -0.5, .0, 0.25, 0.5,
				 		 0.75, 1.0} \\
				 		 & deformation factor   & {0, 0.2, 0.4, 0.5, 0.6, 0.8,
				 		 1.0} \\
 		SMOTE            & number of neighbors & {3, 5} \\
		BORDERLINE SMOTE & number of neighbors & {3, 5} \\
		ADASYN           & number of neighbors & {2, 3} \\
		\bottomrule
	\end{tabular}
	\caption{\label{tab:grid}User-defined grid values}
\end{table}

\subsection{Software implementation}

The implementation of the experimental procedure was based on the Python
programming language, using the Scikit-Learn \cite{Pedregosa2011}, Imbalanced-
Learn \cite{JMLR:v18:16-365} and Geometric-SMOTE \cite{Douzas2019} libraries.
The experiments reported in this paper as well as the analysis of the results
are reproducible using the scripts available at \url{https://github.com/AlgoWit/
publications}.

\section{Results and discussion}

This section presents the results of oversamplers comparison on the LUCAS
dataset. For each combination of classifier and metric, a cross-validation score
is provided in Table \ref{tab:mean_cross_val}:

\pgfplotstabletypeset[
begin table=\begin{longtable},
end table=\end{longtable},
col sep=comma,
header=true,
columns={Classifier,Metric,NO OS,RAND OS,SMOTE,B-SMOTE,ADASYN,G-SMOTE},
columns/Classifier/.style={column type=l,string type},
columns/Metric/.style={column type=l,string type},
columns/NO OS/.style={column type=l,string type},
columns/RAND OS/.style={column type=l,string type},
columns/SMOTE/.style={column type=l,string type},
columns/B-SMOTE/.style={column type=l,string type},
columns/ADASYN/.style={column type=l,string type},
columns/G-SMOTE/.style={column type=l,string type},
every head row/.style={before row=\toprule, after row=\midrule\endhead},
every last row/.style={after row=\bottomrule
\caption{\label{tab:mean_cross_val}Results for cross validation scores of
oversamplers.}}
]
{../analysis/mean_scores.csv}

From this table we can observe that G-SMOTE outperforms all other oversampling
methods for both \textit{G-mean} and \textit{F-score} metrics on all
classifiers. The absolute best results are achieved when G-SMOTE is combined
with LR and RF. It is important to notice that the accuracy scores reflect the
well known bias towards the majority class as discussed in \ref{Evaluation
metrics}. Accordingly, models with no oversampling present the highest accuracy
score. In a multi-class classification problem with an imbalanced dataset, where
the prediction of all the classes are of equal importance as in many remote
sensing applications, overall accuracy should be of secondary importance
compared to more robust metrics, such as \textit{G-mean} and \textit{F-score}
utilized in remote sensing. Nevertheless, even for accuracy metric, G-SMOTE
shows the best performance amongst the oversamplers.

A ranking score was assigned to each oversampling method with the best and worst
performing methods receiving scores equal to 1 and 6, respectively. This
comparison is presented in Table \ref{tab:ranking}:

\pgfplotstabletypeset[
begin table=\begin{longtable},
end table=\end{longtable},
col sep=comma,
header=true,
columns={Classifier,Metric,NO OS,RAND OS,SMOTE,B-SMOTE,ADASYN,G-SMOTE},
columns/Classifier/.style={column type=l,string type},
columns/Metric/.style={column type=l,string type},
columns/NO OS/.style={column type=l,string type},
columns/RAND OS/.style={column type=l,string type},
columns/SMOTE/.style={column type=l,string type},
columns/B-SMOTE/.style={column type=l,string type},
columns/ADASYN/.style={column type=l,string type},
columns/G-SMOTE/.style={column type=l,string type},
every head row/.style={before row=\toprule, after row=\midrule\endhead},
every last row/.style={after row=\bottomrule
\caption{\label{tab:ranking}Results for ranking of oversamplers}}
]
{../analysis/mean_ranking.csv}

From table \ref{tab:ranking} the consistency of G-SMOTE' performance is
confirmed. Table \ref{tab:mean_rankings} was created by averaging the ranking
scores over all classifiers per evaluation metric:

\pgfplotstabletypeset[
begin table=\begin{longtable},
end table=\end{longtable},
col sep=comma,
header=true,
columns={Oversampler,F1 macro,G-mean macro,Accuracy},
columns/Oversampler/.style={column type=l,string type},
columns/F1 macro/.style={column type=l,string type},
columns/G-mean macro/.style={column type=l,string type},
columns/Accuracy/.style={column type=l,string type},
every head row/.style={before row=\toprule, after row=\midrule\endhead},
every last row/.style={after row=\bottomrule
\caption{\label{tab:mean_rankings}Results for mean ranking of oversamplers}}
]
{../analysis/model_mean_ranking.csv}

Table \ref{tab:smote_gsmote} presents the percentage difference between
G-SMOTE and SMOTE:

\pgfplotstabletypeset[
begin table=\begin{longtable},
end table=\end{longtable},
col sep=comma,
header=true,
columns={Classifier,Metric,Difference},
columns/Classifier/.style={column type=l,string type},
columns/Metric/.style={column type=l,string type},
columns/Difference/.style={column type=l,string type},
every head row/.style={before row=\toprule, after row=\midrule\endhead},
every last row/.style={after row=\bottomrule
\caption{\label{tab:smote_gsmote}Results for percentage difference
between G-SMOTE and SMOTE}}
]
{../analysis/mean_perc_diff_scores.csv}

Given the popularity of SMOTE in remote sensing, it makes sense to directly
compare both oversamplers. The table shows that G-SMOTE outperforms SMOTE
regardless of the classifier or metric used.

\section{Conclusions}

In this paper we applied G-SMOTE, a novel oversampling algorithm, on a LULC
production based on the LUCAS, a highly imbalanced multi-class dataset.
Geometric-SMOTE's performance was evaluated and compared with other oversampling
methods. More specifically, Random Oversampling, SMOTE, Borderline-SMOTE and
ADASYN, using Logistic Regression, K-Nearest Neighbors, Decision Trees, Gradient
Boosting Classifier and Random Forest.

The experiment results show the importance of oversampling methods for
multi-class imbalanced data classification as in most of the cases balancing
data with oversampling resulted in higher \textit{G-mean} and \text{F-score}
values.

G-SMOTE can a be a useful tool for remote sensing researchers and practitioners,
as it systematically outperforms the previous widely used oversamplers, such as
Random Oversampling and SMOTE. G-SMOTE is easily accessible to the users through
an open source implementation.

\vspace{6pt}

\authorcontributions{Conceptualization, F.B.; Methodology, G.D.; Software,
G.D.; Validation, F.B., G.D.; Formal Analysis, J.F and M.K.; Writing -
Original Draft Preparation, M.K., J.F.; Writing - Review \& Editing, F.B., G.D.,
J.F., M.K.; Supervision, F.B.; Funding Acquisition, F.B.}

\funding{This research was funded by NAME OF FUNDER grant number XXX.}

\acknowledgments{The authors would like to thank Direção Geral do Território
(DGT) for supporting the used data in this study.}

\conflictsofinterest{The authors declare no conflict of interest. The funders
had no role in the design of the study; in the collection, analyses, or
interpretation of data; in the writing of the manuscript, or in the decision to
publish the results.}

\abbreviations{The following abbreviations are used in this manuscript:\\
\noindent
\begin{tabular}{@{}ll}
  OS & Oversampling\\
  CV & Cross-Validation\\
  LULC & Land Use/Land Cover\\
  LUCAS & Land Use/Cover Area Statistical Survey\\
  SMOTE & Synthetic Minority Over-sampling Technique\\
  ADASYN & Adaptive Synthetic Sampling Technique\\
  LR & Logistic Regression\\
  KNN & K-Nearest Neighbors\\
  DT & Decision Trees\\
  GBC & Gradient Boosting Classifier\\
  RF & Random Forest\\
\end{tabular}}


\reftitle{References}

\externalbibliography{yes}
\bibliography{references}

\end{document}
